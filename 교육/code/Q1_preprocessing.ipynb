{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOPUCcdk2DCiZK9mJTQ6N+5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5b40d706","executionInfo":{"status":"ok","timestamp":1769589537273,"user_tz":-540,"elapsed":15422,"user":{"displayName":"Seoyoon Kim","userId":"14030446416196988450"}},"outputId":"1b7e45e1-adef-4d0e-824d-b61d21845774"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import os\n","import zipfile\n","import pandas as pd\n","import numpy as np\n","from collections import defaultdict\n","\n","# -----------------------------\n","# Welford: mean/variance online\n","# -----------------------------\n","class Welford:\n","    __slots__ = (\"n\", \"mean\", \"M2\")\n","    def __init__(self):\n","        self.n = 0\n","        self.mean = 0.0\n","        self.M2 = 0.0\n","\n","    def update_array(self, x: np.ndarray):\n","        for v in x:\n","            if pd.isna(v):\n","                continue\n","            self.n += 1\n","            delta = v - self.mean\n","            self.mean += delta / self.n\n","            delta2 = v - self.mean\n","            self.M2 += delta * delta2\n","\n","    def variance(self):\n","        if self.n < 2:\n","            return np.nan\n","        return self.M2 / (self.n - 1)\n","\n","# -----------------------------\n","# Utility\n","# -----------------------------\n","LETTER_TO_IDX = {\"a\": 0, \"b\": 1, \"c\": 2, \"d\": 3}\n","\n","def safe_entropy(p: np.ndarray) -> float:\n","    p = p[p > 0]\n","    if p.size <= 1:\n","        return 0.0\n","    return float(-(p * np.log(p)).sum())\n","\n","# -----------------------------\n","# Main\n","# -----------------------------\n","def build_item_master_table(\n","    zip_path: str,\n","    questions_csv_path: str,\n","    irt_b_csv_path: str,\n","    confusion_csv_path: str,\n","    out_dir: str,\n","    out_prefix: str = \"toeic_item\",\n","    chunksize: int = 200_000,\n","    part_to_K: dict | None = None,\n","):\n","    os.makedirs(out_dir, exist_ok=True)\n","\n","    # Part 2만 3지선다\n","    if part_to_K is None:\n","        part_to_K = {1:4, 2:3, 3:4, 4:4, 5:4, 6:4, 7:4}\n","\n","    # -----------------------------\n","    # 1) questions.csv\n","    # -----------------------------\n","    q = pd.read_csv(questions_csv_path)\n","    if \"question_id\" not in q.columns:\n","        q = q.rename(columns={\"item_id\": \"question_id\"})\n","\n","    q[\"question_id\"] = q[\"question_id\"].astype(str).str.strip()\n","\n","    if \"tags\" in q.columns:\n","        q[\"n_tags\"] = q[\"tags\"].fillna(\"\").astype(str).apply(\n","            lambda s: 0 if s.strip() == \"\" else len(s.split(\";\"))\n","        )\n","    else:\n","        q[\"n_tags\"] = np.nan\n","\n","    q[\"part\"] = pd.to_numeric(q[\"part\"], errors=\"ignore\")\n","    q_meta = q[[\"question_id\", \"part\", \"n_tags\"]].drop_duplicates(\"question_id\")\n","\n","    # -----------------------------\n","    # 2) KT1 zip → item aggregation\n","    # -----------------------------\n","    attempts = defaultdict(int)\n","    choice_cnt = defaultdict(lambda: np.zeros(4, dtype=np.int64))\n","    rt_stats = defaultdict(Welford)\n","\n","    with zipfile.ZipFile(zip_path, \"r\") as z:\n","        names = [n for n in z.namelist() if n.endswith(\".csv\")]\n","        print(\"CSV files in zip:\", len(names))\n","\n","        for i, name in enumerate(names, 1):\n","            with z.open(name) as f:\n","                for chunk in pd.read_csv(\n","                    f,\n","                    chunksize=chunksize,\n","                    usecols=[\"question_id\", \"user_answer\", \"elapsed_time\"],\n","                ):\n","                    chunk[\"question_id\"] = chunk[\"question_id\"].astype(str).str.strip()\n","                    chunk[\"user_answer\"] = chunk[\"user_answer\"].astype(str).str.lower()\n","\n","                    # attempts\n","                    for qid, cnt in chunk[\"question_id\"].value_counts().items():\n","                        attempts[qid] += int(cnt)\n","\n","                    # choice counts\n","                    sub = chunk[chunk[\"user_answer\"].isin([\"a\",\"b\",\"c\",\"d\"])]\n","                    gb = sub.groupby([\"question_id\", \"user_answer\"]).size()\n","                    for (qid, ans), cnt in gb.items():\n","                        choice_cnt[qid][LETTER_TO_IDX[ans]] += int(cnt)\n","\n","                    # RT\n","                    rt = pd.to_numeric(chunk[\"elapsed_time\"], errors=\"coerce\")\n","                    for qid, g in rt.groupby(chunk[\"question_id\"]):\n","                        rt_stats[qid].update_array(g.to_numpy())\n","\n","            if i % 5000 == 0:\n","                print(f\"processed {i}/{len(names)} files...\")\n","\n","    rows = []\n","    for item_id, n in attempts.items():\n","        wf = rt_stats[item_id]\n","        cnts = choice_cnt[item_id]\n","        rows.append({\n","            \"item_id\": item_id,\n","            \"n_attempts\": n,\n","            \"choice_cnt_a\": cnts[0],\n","            \"choice_cnt_b\": cnts[1],\n","            \"choice_cnt_c\": cnts[2],\n","            \"choice_cnt_d\": cnts[3],\n","            \"rt_mean\": wf.mean if wf.n > 0 else np.nan,\n","            \"rt_var\": wf.variance(),\n","        })\n","    item_df = pd.DataFrame(rows)\n","\n","    # -----------------------------\n","    # 3) merge question meta\n","    # -----------------------------\n","    item_df = item_df.merge(\n","        q_meta, left_on=\"item_id\", right_on=\"question_id\", how=\"left\"\n","    ).drop(columns=\"question_id\")\n","\n","    # -----------------------------\n","    # 4) part-aware rates\n","    # -----------------------------\n","    def calc_rates(row):\n","        K = part_to_K.get(row[\"part\"], 4)\n","        cnts = np.array([\n","            row[\"choice_cnt_a\"],\n","            row[\"choice_cnt_b\"],\n","            row[\"choice_cnt_c\"],\n","            row[\"choice_cnt_d\"],\n","        ], dtype=float)[:K]\n","        tot = cnts.sum()\n","        if tot == 0:\n","            return pd.Series([K] + [np.nan]*6)\n","\n","        p = cnts / tot\n","        return pd.Series([\n","            K,\n","            p[0] if K > 0 else np.nan,\n","            p[1] if K > 1 else np.nan,\n","            p[2] if K > 2 else np.nan,\n","            p[3] if K > 3 else np.nan,\n","            p.max(),\n","            safe_entropy(p) / np.log(K)\n","        ])\n","\n","    item_df[\n","        [\"K_part\",\n","         \"choice_rate_a\",\"choice_rate_b\",\"choice_rate_c\",\"choice_rate_d\",\n","         \"dominant_choice_rate\",\"choice_entropy_norm\"]\n","    ] = item_df.apply(calc_rates, axis=1)\n","\n","    # -----------------------------\n","    # 5) IRT merge\n","    # -----------------------------\n","    irt = pd.read_csv(irt_b_csv_path).rename(columns={\"b\": \"irt_b\"})\n","    irt[\"item_id\"] = irt[\"item_id\"].astype(str)\n","    item_df = item_df.merge(irt, on=\"item_id\", how=\"left\")\n","\n","    # -----------------------------\n","    # 6) confusion merge (q_* 전부)\n","    # -----------------------------\n","    conf = pd.read_csv(confusion_csv_path)\n","    conf = conf.rename(columns={\"question_id\": \"item_id\"})\n","    conf[\"item_id\"] = conf[\"item_id\"].astype(str)\n","\n","    conf_cols = [\n","        \"item_id\",\n","        \"q_n_attempt\",\n","        \"q_accuracy\",\n","        \"q_confusion_mean\",\n","        \"q_confusion_median\",\n","        \"q_confusion_rate\",\n","        \"q_rt_median\",\n","        \"q_change_rate\",\n","        \"q_change_mean\",\n","    ]\n","    item_df = item_df.merge(conf[conf_cols], on=\"item_id\", how=\"left\")\n","\n","    # -----------------------------\n","    # save\n","    # -----------------------------\n","    out_path = os.path.join(out_dir, f\"{out_prefix}_item_master.csv\")\n","    item_df.to_csv(out_path, index=False)\n","    print(\"saved:\", out_path)\n","    print(\"columns:\", list(item_df.columns))\n","\n","    return item_df\n","\n","\n","# -----------------------------\n","# 실행 (너 경로 기준)\n","# -----------------------------\n","ZIP_PATH = \"/content/drive/MyDrive/EdNET_KT/KT1.zip\"\n","QUESTIONS = \"/content/drive/MyDrive/EdNET_KT/questions.csv\"\n","IRT_B = \"/content/drive/MyDrive/EdNET_KT/part125_final_optimized/item_b_optimized.csv\"\n","\n","CONF_DRIVE = \"/content/drive/MyDrive/EdNET_KT/question_level_confusion (1).csv\"\n","CONF_UPLOADED = \"/mnt/data/question_level_confusion (1).csv\"  # 혹시 Drive에서 못 읽을 때 fallback\n","\n","OUT_DIR = \"/content/drive/MyDrive/EdNET_KT/item_master_outputs\"\n","OUT_PREFIX = \"toeic_item\"\n","\n","df_master = build_item_master_table(\n","    zip_path=ZIP_PATH,\n","    questions_csv_path=QUESTIONS,\n","    irt_b_csv_path=IRT_B,\n","    confusion_csv_path=CONF_DRIVE,   # ← 하나만 넣으면 됨\n","    out_dir=OUT_DIR,\n","    out_prefix=OUT_PREFIX,\n","    part_to_K={1:4, 2:3, 3:4, 4:4, 5:4, 6:4, 7:4}\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1jtXwmODvIIW","executionInfo":{"status":"ok","timestamp":1769591043762,"user_tz":-540,"elapsed":436207,"user":{"displayName":"Seoyoon Kim","userId":"14030446416196988450"}},"outputId":"830da805-7977-4c36-d0ec-e9472c0e8648"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-3242818330.py:78: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n","  q[\"part\"] = pd.to_numeric(q[\"part\"], errors=\"ignore\")\n"]},{"output_type":"stream","name":"stdout","text":["CSV files in zip: 23477\n","processed 5000/23477 files...\n","processed 10000/23477 files...\n","processed 15000/23477 files...\n","processed 20000/23477 files...\n","saved: /content/drive/MyDrive/EdNET_KT/item_master_outputs/toeic_item_item_master.csv\n","columns: ['item_id', 'n_attempts', 'choice_cnt_a', 'choice_cnt_b', 'choice_cnt_c', 'choice_cnt_d', 'rt_mean', 'rt_var', 'part', 'n_tags', 'K_part', 'choice_rate_a', 'choice_rate_b', 'choice_rate_c', 'choice_rate_d', 'dominant_choice_rate', 'choice_entropy_norm', 'irt_b', 'q_n_attempt', 'q_accuracy', 'q_confusion_mean', 'q_confusion_median', 'q_confusion_rate', 'q_rt_median', 'q_change_rate', 'q_change_mean']\n"]}]}]}
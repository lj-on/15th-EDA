import pandas as pd
import glob
import os
import ast
import re
import numpy as np
import matplotlib.pyplot as plt
from gensim.models import Word2Vec
from sklearn.manifold import TSNE
from wordcloud import WordCloud
from matplotlib import font_manager, rc
import matplotlib.pyplot as plt


# 1. ì„¤ì • ë° ë¶ˆìš©ì–´ ì •ì˜
STOPWORDS = {
    'í•˜ë‹¤', 'ë˜ë‹¤', 'ìˆë‹¤', 'ì—†ë‹¤', 'ê°™ë‹¤', 'ë³´ë‹¤', 'ì£¼ë‹¤', 'ì˜¤ë‹¤', 'ê°€ë‹¤',
    'ì´ë‹¤', 'ì•„ë‹ˆë‹¤', 'ì•Šë‹¤', 'ëª»í•˜ë‹¤', 'ì‹¶ë‹¤', 'ì•Œë‹¤', 'ëª¨ë¥´ë‹¤',
    'ê·¸ë ‡ë‹¤', 'ì–´ë–»ë‹¤', 'ì´ë ‡ë‹¤', 'ì €ë ‡ë‹¤',
    'ì €', 'ê·¸', 'ì´ê²ƒ', 'ê·¸ê²ƒ', 'ì €ê²ƒ',
    'ë•Œ', 'ê²ƒ', 'ìˆ˜', 'ë“¯', 'ë”', 'ë˜', 'ë‹¤ì‹œ', 'ë„ˆë¬´', 'ì •ë§', 'ì§„ì§œ','ì›Œìš°ì›Œ','ì•„ë‹ˆì•¼'
}

# 2. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ (ì‚¬ìš©ì ì‘ì„± ì½”ë“œ í¬í•¨)
def is_onomatopoeia(word):
    """ì˜ì„±ì–´/ë°˜ë³µ ì–´êµ¬ ê°ì§€"""
    if len(word) < 2: return False
    if len(set(word)) == 1: return True
    if len(word) >= 4 and len(word) % 2 == 0:
        half = len(word) // 2
        if word[:half] == word[half:]: return True
    return False

def is_valid_korean(word):
    """ìœ íš¨í•œ í•œê¸€ ë‹¨ì–´ì¸ì§€ í™•ì¸"""
    if not word or len(word) < 2: return False
    has_korean = any('ê°€' <= c <= 'í£' for c in word)
    if not has_korean: return False
    if is_onomatopoeia(word): return False
    if word in STOPWORDS: return False
    return True

def parse_tokens(token_str):
    """ë¬¸ìì—´ í˜•íƒœì˜ í† í° ë¦¬ìŠ¤íŠ¸ë¥¼ íŒŒì‹± ë° í•„í„°ë§"""
    if pd.isna(token_str): return []
    try:
        # ì—‘ì…€ì˜ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ë¥¼ ì‹¤ì œ ê°ì²´ë¡œ ë³€í™˜
        tokens = ast.literal_eval(token_str)
        return [t for t in tokens if is_valid_korean(t)]
    except:
        return []

def normalize_gender_value(gender):
    """ì„±ë³„ ë°ì´í„° ì •ê·œí™” (í•„ìš”ì— ë”°ë¼ ìˆ˜ì • ê°€ëŠ¥)"""
    gender = str(gender).lower()
    if 'female' in gender or 'ì—¬' in gender: return 'female'
    if 'male' in gender or 'ë‚¨' in gender: return 'male'
    return 'mixed/unknown'

#3
def load_and_preprocess_kpop(lyrics_dir):
    # 1. ~$ë¡œ ì‹œì‘í•˜ëŠ” ì„ì‹œ íŒŒì¼ì€ ì œì™¸í•˜ê³  ì‹¤ì œ ì—‘ì…€ íŒŒì¼ë§Œ ë¦¬ìŠ¤íŠ¸ì—…
    all_files = [f for f in glob.glob(os.path.join(lyrics_dir, '*.xlsx')) 
                 if not os.path.basename(f).startswith('~$')]
    
    if not all_files:
        print(f"âŒ ê²½ë¡œ ë‚´ì— ìœ íš¨í•œ ì—‘ì…€ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {lyrics_dir}")
        return None

    df_list = []
    for file in all_files:
        try:
            # 2. engine='openpyxl'ì„ ëª…ì‹œì ìœ¼ë¡œ ì§€ì •í•˜ì—¬ ì—ëŸ¬ ë°©ì§€
            df = pd.read_excel(file, engine='openpyxl')
            df_list.append(df)
            print(f"  - ë¡œë“œ ì™„ë£Œ: {os.path.basename(file)}")
        except Exception as e:
            # ì–´ë–¤ íŒŒì¼ì—ì„œ ë¬¸ì œê°€ ìƒê²¼ëŠ”ì§€ ì¶œë ¥í•˜ê³  ë‹¤ìŒ íŒŒì¼ë¡œ ì§„í–‰
            print(f"âš ï¸ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ ({os.path.basename(file)}): {e}")

    if not df_list:
        return None

    combined_df = pd.concat(df_list, ignore_index=True)
    
    # ì¤‘ë³µ ì œê±° (Song_ID ê¸°ì¤€)
    if 'Song_ID' in combined_df.columns:
        combined_df = combined_df.drop_duplicates(subset=['Song_ID'], keep='first')
    
    # ì„±ë³„ ì •ê·œí™”
    if 'Artist_Gender' in combined_df.columns:
        combined_df['Gender'] = combined_df['Artist_Gender'].apply(normalize_gender_value)
    
    print("â³ ê°€ì‚¬ í† í° ì •ì œ ì¤‘...")
    # Processed_Tokens ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ í›„ ì²˜ë¦¬
    if 'Processed_Tokens' in combined_df.columns:
        combined_df['Clean_Tokens'] = combined_df['Processed_Tokens'].apply(parse_tokens)
    else:
        print("âŒ 'Processed_Tokens' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return combined_df
    
    combined_df['Total_Words'] = combined_df['Clean_Tokens'].apply(len)
    combined_df['Unique_Words'] = combined_df['Clean_Tokens'].apply(lambda x: len(set(x)))
    combined_df['Era'] = (combined_df['Year'] // 10 * 10).astype(str) + 's'

    print(f"âœ… ì™„ë£Œ! ì´ {len(combined_df):,}ê°œ ê³¡ ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ.")
    return combined_df

# 4. ì‹¤í–‰
data_path = r'C:\Users\User\Desktop\DSL\eda\kpop_lyrics_analysis-main\final_dataset'
final_df = load_and_preprocess_kpop(data_path)

# ê²°ê³¼ í™•ì¸
if final_df is not None:
    print(final_df[['Year', 'Title', 'Total_Words', 'Era']].head())

# Step 2. ì‹œëŒ€ë³„ ë°ì´í„° ë¶„í•  ë° ëª¨ë¸ í•™ìŠµ
# ============================================================

ERA_DEFS = [
    ('Era1 (1996-2005)', 1996, 2005),
    ('Era2 (2006-2015)', 2006, 2015),
    ('Era3 (2016-2025)', 2016, 2025),
]


def slice_by_era(df):
    """ë°ì´í„°ë¥¼ 3ê°œ ì‹œëŒ€ë¡œ ë¶„í• """
    eras = {
        name: df[(df['Year'] >= start) & (df['Year'] <= end)]
        for name, start, end in ERA_DEFS
    }

    print("\nâœ“ ì‹œëŒ€ë³„ ë°ì´í„° ë¶„í• :")
    for name, data in eras.items():
        print(f"  - {name}: {len(data):,}ê³¡")

    return eras


def slice_by_era_and_gender(df, gender_col='Gender'):
    """ì‹œëŒ€+ì„±ë³„ ë¶„í• """
    groups = {}
    for name, start, end in ERA_DEFS:
        era_df = df[(df['Year'] >= start) & (df['Year'] <= end)]
        if gender_col not in era_df.columns:
            groups[name] = era_df
            continue
        for gender, gdf in era_df.groupby(gender_col):
            label = f"{name} | {gender}"
            groups[label] = gdf

    print("\nâœ“ ì‹œëŒ€+ì„±ë³„ ë°ì´í„° ë¶„í• :")
    for name, data in groups.items():
        print(f"  - {name}: {len(data):,}ê³¡")

    return groups


def train_era_model(era_df, era_name, vector_size=100, window=4,
                    min_count=5, min_doc_count=5):
    """ì‹œëŒ€ë³„ Word2Vec ëª¨ë¸ í•™ìŠµ
    - min_count: ì „ì²´ì—ì„œ ìµœì†Œ ë“±ì¥ íšŸìˆ˜
    - min_doc_count: ìµœì†Œ ë“±ì¥ ê³¡ ìˆ˜
    """
    from collections import Counter

    print(f"\n[{era_name}] ëª¨ë¸ í•™ìŠµ ì‹œì‘...")

    # ì „ì²˜ë¦¬ëœ í† í° íŒŒì‹±
    tokenized_data = []
    for tokens_str in era_df['Processed_Tokens']:
        tokens = parse_tokens(tokens_str)
        if len(tokens) >= 3:
            tokenized_data.append(tokens)

    if len(tokenized_data) < 10:
        print(f"  âš  ë°ì´í„° ë¶€ì¡± (í† í°í™”ëœ ê³¡: {len(tokenized_data)})")
        return None

    # ë‹¨ì–´ë³„ ë“±ì¥ ê³¡ ìˆ˜(document frequency) ê³„ì‚°
    doc_freq = Counter()
    for tokens in tokenized_data:
        unique_tokens = set(tokens)  # ê³¡ ë‚´ ì¤‘ë³µ ì œê±°
        doc_freq.update(unique_tokens)

    # min_doc_count ë¯¸ë§Œì¸ ë‹¨ì–´ ì œê±°
    rare_words = {w for w, cnt in doc_freq.items() if cnt < min_doc_count}
    filtered_data = [
        [w for w in tokens if w not in rare_words]
        for tokens in tokenized_data
    ]
    filtered_data = [t for t in filtered_data if len(t) >= 3]

    before_vocab = len(set(w for tokens in tokenized_data for w in tokens))
    after_vocab = len(set(w for tokens in filtered_data for w in tokens))
    print(f"  - ê³¡ ìˆ˜ í•„í„°(â‰¥{min_doc_count}ê³¡): {before_vocab:,} â†’ {after_vocab:,} ë‹¨ì–´")

    # Word2Vec í•™ìŠµ
    model = Word2Vec(
        sentences=filtered_data,
        vector_size=vector_size,
        window=window,
        min_count=min_count,
        workers=4,
        sg=1,  # Skip-gram
        epochs=10
    )

    print(f"  âœ“ í•™ìŠµ ì™„ë£Œ! (ì–´íœ˜ ìˆ˜: {len(model.wv):,}ê°œ)")
    return model


def train_all_models(eras):
    """ëª¨ë“  ì‹œëŒ€ë³„ ëª¨ë¸ í•™ìŠµ"""
    models = {}
    for name, data in eras.items():
        model = train_era_model(data, name)
        if model:
            models[name] = model
    return models


# ============================================================
# Step 3. ì˜ë¯¸ ë³€í™” ë¶„ì„
# ============================================================

def analyze_semantic_shift(models, target_word, topn=15):
    """íŠ¹ì • ë‹¨ì–´ì˜ ì‹œëŒ€ë³„ ì˜ë¯¸ ë³€í™” ë¶„ì„"""
    print(f"\n{'='*60}")
    print(f"  '{target_word}'ì˜ ì‹œëŒ€ë³„ ì˜ë¯¸ ë³€í™” ë¶„ì„")
    print(f"{'='*60}")

    results = {}

    for name, model in models.items():
        try:
            similar_words = model.wv.most_similar(target_word, topn=topn)
            words_only = [w[0] for w in similar_words]

            results[name] = {
                'similar_words': similar_words,
                'words_list': words_only
            }

            print(f"\n[{name}]")
            print(f"  ìœ ì‚¬ ë‹¨ì–´: {', '.join(words_only[:10])}")

        except KeyError:
            print(f"\n[{name}] '{target_word}'ê°€ ì´ ì‹œëŒ€ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤.")
            results[name] = None

    return results


def find_word_sources(df, target_word, era_range=None, max_results=10):
    """íŠ¹ì • ë‹¨ì–´ê°€ í¬í•¨ëœ ì›ë¬¸ ê°€ì‚¬ ì¶œì²˜ í™•ì¸"""
    print(f"\n{'='*60}")
    print(f"  '{target_word}' ë‹¨ì–´ ì¶œì²˜ ë¶„ì„")
    print(f"{'='*60}")

    if era_range:
        filtered_df = df[(df['Year'] >= era_range[0]) & (df['Year'] <= era_range[1])]
        print(f"  ë¶„ì„ ë²”ìœ„: {era_range[0]} ~ {era_range[1]}ë…„")
    else:
        filtered_df = df

    matches = []
    for idx, row in filtered_df.iterrows():
        tokens_str = str(row.get('Processed_Tokens', ''))
        if target_word in tokens_str:
            context = extract_context(str(row.get('Lyrics', '')), target_word)
            matches.append({
                'Year': row['Year'],
                'Title': row['Title'],
                'Artist': row['Artist'],
                'Context': context
            })

    # ì—°ë„ë³„ ë¹ˆë„
    year_counts = {}
    for m in matches:
        year = m['Year']
        year_counts[year] = year_counts.get(year, 0) + 1

    print(f"\n  ì´ {len(matches)}ê³¡ì—ì„œ '{target_word}' ë°œê²¬")

    if year_counts:
        print(f"\n  [ì—°ë„ë³„ ë¹ˆë„]")
        for year in sorted(year_counts.keys()):
            bar = 'â–ˆ' * min(year_counts[year], 30)
            print(f"  {year}: {bar} ({year_counts[year]}ê³¡)")

    print(f"\n  [ëŒ€í‘œ ê³¡ ëª©ë¡]")
    for i, m in enumerate(matches[:max_results]):
        print(f"  {i+1}. [{m['Year']}] {m['Artist']} - {m['Title']}")
        if m['Context']:
            print(f"      \"{m['Context'][:60]}...\"")

    return matches


def extract_context(lyrics, word, window=30):
    """ë‹¨ì–´ ì£¼ë³€ ë¬¸ë§¥ ì¶”ì¶œ"""
    if not lyrics:
        return ""
    idx = lyrics.find(word)
    if idx == -1:
        return ""
    start = max(0, idx - window)
    end = min(len(lyrics), idx + len(word) + window)
    return lyrics[start:end].replace('\n', ' ')


def deep_dive_word(models, df, target_word, era_name=None):
    """íŠ¹ì • ë‹¨ì–´ ì‹¬ì¸µ ë¶„ì„"""
    print(f"\n{'#'*60}")
    print(f"  '{target_word}' ì‹¬ì¸µ ë¶„ì„")
    print(f"{'#'*60}")

    # ìœ ì‚¬ ë‹¨ì–´ ë¶„ì„
    if era_name and era_name in models:
        model = models[era_name]
        try:
            similar = model.wv.most_similar(target_word, topn=15)
            print(f"\n  [{era_name}] '{target_word}'ì™€ ìœ ì‚¬í•œ ë‹¨ì–´:")
            for w, score in similar:
                print(f"    - {w}: {score:.3f}")
        except KeyError:
            print(f"  '{target_word}'ê°€ í•´ë‹¹ ì‹œëŒ€ì— ì—†ìŠµë‹ˆë‹¤.")
    else:
        for name, model in models.items():
            try:
                similar = model.wv.most_similar(target_word, topn=5)
                words_str = ', '.join([f"{w}({s:.2f})" for w, s in similar])
                print(f"\n  [{name}] {words_str}")
            except KeyError:
                print(f"\n  [{name}] ì—†ìŒ")

    # ì¶œì²˜ í™•ì¸
    if era_name:
        import re
        years = re.findall(r'\d{4}', era_name)
        if len(years) == 2:
            find_word_sources(df, target_word, era_range=(int(years[0]), int(years[1])), max_results=5)
    else:
        find_word_sources(df, target_word, max_results=10)


# ============================================================
# Step 4. t-SNE ì‹œê°í™”
# ============================================================

def setup_korean_font():
    """í•œê¸€ í°íŠ¸ ì„¤ì •"""
    import platform
    system = platform.system()

    if system == 'Darwin':
        fonts = ['AppleGothic', 'Apple SD Gothic Neo']
    elif system == 'Windows':
        fonts = ['Malgun Gothic', 'NanumGothic']
    else:
        fonts = ['NanumGothic', 'UnDotum']

    for font in fonts:
        try:
            plt.rc('font', family=font)
            plt.rcParams['axes.unicode_minus'] = False
            return True
        except:
            continue
    return False


def plot_semantic_field(target_word, models, topn=15, save_path=None):
    """ì‹œëŒ€ë³„ ì˜ë¯¸ì¥ ì‹œê°í™” (ê°œì„ ëœ ë²„ì „)"""
    setup_korean_font()

    # ì‹œëŒ€ë³„ ìƒ‰ìƒ íŒ”ë ˆíŠ¸
    era_colors = ['#E74C3C', '#3498DB', '#9B59B6']

    fig, axes = plt.subplots(1, 3, figsize=(21, 7))
    axes = axes.flatten()

    fig.suptitle(f"'{target_word}'ì˜ ì‹œëŒ€ë³„ ì˜ë¯¸ ë³€í™” (t-SNE)",
                 fontsize=18, fontweight='bold', y=0.98)

    for i, (name, model) in enumerate(models.items()):
        ax = axes[i]
        color = era_colors[i]

        try:
            similar_words = model.wv.most_similar(target_word, topn=topn)
            words = [target_word] + [w[0] for w in similar_words]
            scores = [1.0] + [s for _, s in similar_words]  # ìœ ì‚¬ë„ ì ìˆ˜

            word_vectors = np.array([model.wv[w] for w in words])

            perplexity = min(5, len(words) - 1)
            tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity)
            coords = tsne.fit_transform(word_vectors)

            # ë°°ê²½ ìŠ¤íƒ€ì¼
            ax.set_facecolor('#FAFAFA')

            # ìœ ì‚¬ë„ì— ë”°ë¥¸ ì  í¬ê¸° (ê°€ê¹Œìš¸ìˆ˜ë¡ í¼)
            sizes = [300] + [80 + scores[j] * 150 for j in range(1, len(scores))]

            # ìœ ì‚¬ ë‹¨ì–´ë“¤ (ì—°í•œ ìƒ‰)
            ax.scatter(coords[1:, 0], coords[1:, 1],
                      c=color, s=sizes[1:], alpha=0.4, edgecolors='white', linewidths=1.5)

            # íƒ€ê²Ÿ ë‹¨ì–´ (ì§„í•œ ìƒ‰, ë³„ ëª¨ì–‘)
            ax.scatter(coords[0, 0], coords[0, 1],
                      c=color, s=400, marker='*', edgecolors='black', linewidths=2, zorder=5)

            # ìƒìœ„ 5ê°œë§Œ ë¼ë²¨ í‘œì‹œ (ê²¹ì¹¨ ë°©ì§€)
            for j, txt in enumerate(words[:6]):
                if j == 0:
                    ax.annotate(txt, (coords[j, 0], coords[j, 1]),
                               fontsize=14, fontweight='bold', color='black',
                               ha='center', va='bottom',
                               bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))
                else:
                    ax.annotate(txt, (coords[j, 0], coords[j, 1]),
                               fontsize=10, color='#333333',
                               ha='center', va='bottom')

            # ë‚˜ë¨¸ì§€ ë‹¨ì–´ë“¤ (ì‘ì€ í°íŠ¸)
            for j, txt in enumerate(words[6:], start=6):
                ax.annotate(txt, (coords[j, 0], coords[j, 1]),
                           fontsize=8, color='#666666', alpha=0.7,
                           ha='center', va='bottom')

            # ì‹œëŒ€ ì´ë¦„ (ë…„ë„ë§Œ ì¶”ì¶œ)
            era_short = name.split('(')[1].replace(')', '') if '(' in name else name
            ax.set_title(era_short, fontsize=14, fontweight='bold',
                        color=color, pad=10)

            ax.axis('off')

            # í…Œë‘ë¦¬ ì¶”ê°€
            for spine in ax.spines.values():
                spine.set_visible(True)
                spine.set_color(color)
                spine.set_linewidth(2)

        except KeyError:
            ax.set_facecolor('#F5F5F5')
            ax.text(0.5, 0.5, f"'{target_word}'\në°ì´í„° ì—†ìŒ",
                   ha='center', va='center', fontsize=12, color='#999999')
            era_short = name.split('(')[1].replace(')', '') if '(' in name else name
            ax.set_title(era_short, fontsize=14, fontweight='bold', color='#CCCCCC')
            ax.axis('off')

    for j in range(i + 1, len(axes)):
        axes[j].axis('off')

    plt.tight_layout()
    plt.subplots_adjust(top=0.92)

    if save_path:
        plt.savefig(save_path, dpi=200, bbox_inches='tight', facecolor='white')
        print(f"  âœ“ ì €ì¥: {save_path}")

    plt.close()


# ============================================================
# Step 5. ë¦¬í¬íŠ¸ ìƒì„±
# ============================================================

def trace_similar_word_sources(models, eras_data, df, target_words,
                               output_file='output/word_sources_report.txt'):
    """
    ìœ ì‚¬ ë‹¨ì–´ë³„ ì¶œì²˜ ê³¡ ì¶”ì  ë¦¬í¬íŠ¸ (ì „ì²´ ê³¡ + ì›ë¬¸ ê°€ì‚¬ ê·¼ê±°)
    - ì›ë¬¸ ê°€ì‚¬(Lyrics)ì—ì„œ í•´ë‹¹ ë‹¨ì–´ê°€ ë“±ì¥í•˜ëŠ” ë¬¸ë§¥ì„ ì¶”ì¶œ
    - ëª¨ë“  ê³¡ ëª©ë¡ í¬í•¨
    """
    import re as re_module

    print("\n[ìœ ì‚¬ ë‹¨ì–´ ì¶œì²˜ ì¶”ì ]")

    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("  ìœ ì‚¬ ë‹¨ì–´ ì¶œì²˜ ì¶”ì  ë¦¬í¬íŠ¸ (ì „ê³¡ + ì›ë¬¸ ê·¼ê±°)\n")
        f.write("=" * 80 + "\n")

        for target_word in target_words:
            f.write(f"\n\n{'#' * 80}\n")
            f.write(f"  '{target_word}' ìœ ì‚¬ ë‹¨ì–´ ì¶œì²˜\n")
            f.write(f"{'#' * 80}\n")

            for era_name, model in models.items():
                years = re_module.findall(r'\d{4}', era_name)
                if len(years) != 2:
                    continue
                year_start, year_end = int(years[0]), int(years[1])
                era_df = df[(df['Year'] >= year_start) & (df['Year'] <= year_end)]

                f.write(f"\n\n  [{era_name}]\n")
                f.write(f"  {'=' * 60}\n")

                try:
                    similar = model.wv.most_similar(target_word, topn=10)
                except KeyError:
                    f.write(f"  '{target_word}' ë°ì´í„° ì—†ìŒ\n")
                    continue

                for sim_word, score in similar:
                    # Processed_Tokens + ì›ë¬¸ ê°€ì‚¬(Lyrics) ê²°í•© ê²€ìƒ‰
                    matches = []
                    search_terms = _get_search_stems(sim_word)

                    for _, row in era_df.iterrows():
                        # 1ì°¨: Processed_Tokensì—ì„œ ì›í˜•(lemma) ë§¤ì¹­
                        token_str = row.get('Processed_Tokens', '')
                        tokens = parse_tokens(token_str) if not pd.isna(token_str) else []
                        in_tokens = sim_word in tokens

                        # 2ì°¨: ì›ë¬¸ ê°€ì‚¬ì—ì„œ ì–´ê°„ ê²€ìƒ‰
                        lyrics = str(row.get('Lyrics', ''))
                        if pd.isna(lyrics):
                            lyrics = ''
                        found_term = None
                        for st in search_terms:
                            if _find_with_boundary(lyrics, st) != -1:
                                found_term = st
                                break

                        if not in_tokens and not found_term:
                            continue

                        # ë¬¸ë§¥ ì¶”ì¶œ (ì›ë¬¸ì—ì„œ)
                        contexts = []
                        if found_term:
                            contexts = _extract_all_contexts(lyrics, found_term)
                        elif in_tokens and lyrics:
                            # í† í°ì—ëŠ” ìˆì§€ë§Œ ì–´ê°„ ê²€ìƒ‰ìœ¼ë¡  ëª» ì°¾ì€ ê²½ìš°
                            # â†’ ì›í˜•ìœ¼ë¡œ í•œë²ˆ ë” ì‹œë„
                            if _find_with_boundary(lyrics, sim_word) != -1:
                                contexts = _extract_all_contexts(lyrics, sim_word)

                        matches.append({
                            'Year': row['Year'],
                            'Month': row.get('Month', ''),
                            'Rank': row.get('Rank', ''),
                            'Title': row['Title'],
                            'Artist': row['Artist'],
                            'Contexts': contexts
                        })

                    f.write(f"\n  â–  {sim_word} (ìœ ì‚¬ë„: {score:.3f}) - ì´ {len(matches)}ê³¡\n")
                    f.write(f"  {'-' * 55}\n")

                    if not matches:
                        f.write(f"    (ì›ë¬¸ì—ì„œ ë°œê²¬ë˜ì§€ ì•ŠìŒ)\n")
                        continue

                    for m in matches:
                        rank_info = f"#{m['Rank']}" if m['Rank'] else ''
                        f.write(f"    [{m['Year']}] {m['Artist']} - {m['Title']} {rank_info}\n")
                        if m['Contexts']:
                            for ctx in m['Contexts'][:2]:
                                f.write(f"      â†’ \"{ctx}\"\n")
                        else:
                            f.write(f"      â†’ (í† í° ë§¤ì¹­, ì›ë¬¸ ë¬¸ë§¥ ë¯¸ë°œê²¬)\n")

                    f.write("\n")

            print(f"  âœ“ '{target_word}' ì¶œì²˜ ì¶”ì  ì™„ë£Œ")

    print(f"âœ“ ì¶œì²˜ ë¦¬í¬íŠ¸ ì €ì¥: {output_file}")


def _is_korean_syllable(ch):
    """í•œê¸€ ìŒì ˆ(ê°€~í£) ì—¬ë¶€ í™•ì¸"""
    return '\uAC00' <= ch <= '\uD7A3'


def _find_with_boundary(text, word, start=0):
    """
    í…ìŠ¤íŠ¸ì—ì„œ ë‹¨ì–´ë¥¼ ì°¾ë˜, ì• ê¸€ìê°€ í•œê¸€ì´ ì•„ë‹Œ ê²½ìš°ë§Œ ë§¤ì¹­
    (ì˜ˆ: 'êµ°ëŒ€'ë¥¼ ì°¾ì„ ë•Œ 'ìˆ˜êµ°ëŒ€'ëŠ” ë§¤ì¹­í•˜ì§€ ì•ŠìŒ)
    """
    while True:
        idx = text.find(word, start)
        if idx == -1:
            return -1
        # ì• ê¸€ì ê²½ê³„ í™•ì¸: ì• ê¸€ìê°€ í•œê¸€ ìŒì ˆì´ë©´ ë‹¤ë¥¸ ë‹¨ì–´ì˜ ì¼ë¶€
        if idx > 0 and _is_korean_syllable(text[idx - 1]):
            start = idx + 1
            continue
        return idx


def _get_search_stems(word):
    """
    ë™ì‚¬/í˜•ìš©ì‚¬ì˜ ì–´ê°„(stem)ì„ ì¶”ì¶œí•˜ì—¬ ê²€ìƒ‰ì–´ ëª©ë¡ ìƒì„±
    ì˜ˆ: 'ì•“ë‹¤' â†’ ['ì•“ë‹¤', 'ì•“ì•„', 'ì•“ê³ ', 'ì•“']
        'í—¤ì•„ë¦¬ë‹¤' â†’ ['í—¤ì•„ë¦¬ë‹¤', 'í—¤ì•„ë¦¬', 'í—¤ì•„ë ¤', 'í—¤ì•„ë¦´']
        'ê¸°ì•½' (ëª…ì‚¬) â†’ ['ê¸°ì•½']
    """
    stems = [word]  # ì›í˜• ê·¸ëŒ€ë¡œë„ í¬í•¨

    # '~ë‹¤'ë¡œ ëë‚˜ëŠ” ë™ì‚¬/í˜•ìš©ì‚¬ â†’ ì–´ê°„ ì¶”ì¶œ
    if word.endswith('ë‹¤') and len(word) >= 2:
        stem = word[:-1]  # 'ì•“ë‹¤' â†’ 'ì•“', 'í—¤ì•„ë¦¬ë‹¤' â†’ 'í—¤ì•„ë¦¬'
        stems.append(stem)

        # ì£¼ìš” í™œìš© ì–´ë¯¸ ì¶”ê°€
        last_char = stem[-1] if stem else ''
        if last_char:
            # ë°›ì¹¨ ìœ ë¬´ì— ë”°ë¼ ë‹¤ë¥¸ í™œìš©í˜•
            code = ord(last_char) - 0xAC00
            if code >= 0:
                jong = code % 28  # ì¢…ì„± (0ì´ë©´ ë°›ì¹¨ ì—†ìŒ)
                if jong == 0:  # ë°›ì¹¨ ì—†ìŒ: ê°€ë‹¤â†’ê°€, ì„œë‹¤â†’ì„œ
                    stems.append(stem + 'ê³ ')
                    stems.append(stem + 'ì„œ')
                    stems.append(stem + 'ì§€')
                    stems.append(stem + 'ã„¹')
                else:  # ë°›ì¹¨ ìˆìŒ: ì•“â†’ì•“ì•„, ë¨¹â†’ë¨¹ì–´
                    stems.append(stem + 'ì•„')
                    stems.append(stem + 'ì–´')
                    stems.append(stem + 'ê³ ')
                    stems.append(stem + 'ì€')
                    stems.append(stem + 'ì„')
    else:
        # ëª…ì‚¬: ê·¸ëŒ€ë¡œ + ì• 2ê¸€ì (3ê¸€ì ì´ìƒì¼ ë•Œ)
        if len(word) >= 3:
            stems.append(word[:2])

    return stems


def _extract_all_contexts(lyrics, word, window=25, max_contexts=2):
    """ì›ë¬¸ ê°€ì‚¬ì—ì„œ ë‹¨ì–´ê°€ ë“±ì¥í•˜ëŠ” ëª¨ë“  ë¬¸ë§¥ ì¶”ì¶œ (ë‹¨ì–´ ê²½ê³„ í™•ì¸)"""
    contexts = []
    start = 0
    while True:
        idx = _find_with_boundary(lyrics, word, start)
        if idx == -1:
            break
        s = max(0, idx - window)
        e = min(len(lyrics), idx + len(word) + window)
        ctx = lyrics[s:e].replace('\n', ' ').strip()
        if s > 0:
            ctx = '...' + ctx
        if e < len(lyrics):
            ctx = ctx + '...'
        contexts.append(ctx)
        start = idx + len(word)
        if len(contexts) >= max_contexts:
            break
    return contexts

    print(f"âœ“ ì¶œì²˜ ë¦¬í¬íŠ¸ ì €ì¥: {output_file}")


def _write_report_header(f, title):
    f.write("=" * 70 + "\n")
    f.write(f"  {title}\n")
    f.write("=" * 70 + "\n\n")


def generate_report(models, target_words, output_file='analysis_report.txt',
                    title="K-POP ê°€ì‚¬ ì‹œëŒ€ë³„ ì˜ë¯¸ ë³€í™” ë¶„ì„ ë¦¬í¬íŠ¸",
                    mode='w'):
    """ë¶„ì„ ê²°ê³¼ í…ìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ìƒì„±"""
    with open(output_file, mode, encoding='utf-8') as f:
        if mode == 'w':
            _write_report_header(f, title)
        else:
            f.write("\n")
            _write_report_header(f, title)

        for word in target_words:
            f.write(f"\nâ–  '{word}'ì˜ ì‹œëŒ€ë³„ ì˜ë¯¸ ë³€í™”\n")
            f.write("-" * 50 + "\n")

            for era_name, model in models.items():
                try:
                    similar = model.wv.most_similar(word, topn=10)
                    words_str = ', '.join([w[0] for w in similar])
                    f.write(f"\n[{era_name}]\n  {words_str}\n")
                except KeyError:
                    f.write(f"\n[{era_name}] í•´ë‹¹ ë‹¨ì–´ ì—†ìŒ\n")

    print(f"\nâœ“ ë¦¬í¬íŠ¸ ì €ì¥: {output_file}")


def _extract_gender_from_key(model_key):
    if '|' not in model_key:
        return None
    return model_key.split('|', 1)[1].strip()


def _filter_models_by_gender(models, gender):
    return {
        name: model for name, model in models.items()
        if _extract_gender_from_key(name) == gender
    }


def _get_genders_from_models(models):
    genders = set()
    for name in models.keys():
        g = _extract_gender_from_key(name)
        if g:
            genders.add(g)
    return sorted(genders)


def append_gender_report(base_report_file, gender_models, target_words):
    """analysis_report.txtì— ì„±ë³„ ì„¹ì…˜ ì¶”ê°€"""
    if not gender_models:
        return
    generate_report(
        gender_models,
        target_words,
        output_file=base_report_file,
        title="ì„±ë³„(ì‹œëŒ€+ì„±ë³„) ì˜ë¯¸ ë³€í™” ë¶„ì„",
        mode='a'
    )


def generate_gender_reports(gender_models, target_words, output_dir='output'):
    """ì„±ë³„ë³„ ë¦¬í¬íŠ¸ íŒŒì¼ ìƒì„±"""
    genders = _get_genders_from_models(gender_models)
    for gender in genders:
        models = _filter_models_by_gender(gender_models, gender)
        if not models:
            continue
        output_file = os.path.join(output_dir, f'analysis_report_{gender}.txt')
        title = f"K-POP ê°€ì‚¬ ì˜ë¯¸ ë³€í™” ë¶„ì„ ë¦¬í¬íŠ¸ ({gender})"
        generate_report(models, target_words, output_file, title=title, mode='w')


def generate_gender_tsne_plots(gender_models, target_words, output_dir='output'):
    """ì„±ë³„ ê¸°ë°˜ t-SNE ì‹œê°í™” ìƒì„±"""
    genders = _get_genders_from_models(gender_models)
    for gender in genders:
        models = _filter_models_by_gender(gender_models, gender)
        if not models:
            continue
        for word in target_words:
            try:
                save_path = os.path.join(output_dir, f'tsne_{word}_{gender}.png')
                plot_semantic_field(word, models, save_path=save_path)
            except Exception as e:
                print(f"  âš  '{word}' ({gender}) ì‹œê°í™” ì‹¤íŒ¨: {e}")


def generate_gender_word_sources(gender_models, df, target_words, output_dir='output'):
    """ì„±ë³„ ê¸°ë°˜ word_sources ë¦¬í¬íŠ¸ ìƒì„±"""
    if not gender_models:
        return
    trace_similar_word_sources(
        gender_models,
        None,
        df,
        target_words,
        os.path.join(output_dir, 'word_sources_report_gender.txt')
    )

    genders = _get_genders_from_models(gender_models)
    for gender in genders:
        models = _filter_models_by_gender(gender_models, gender)
        if not models:
            continue
        output_file = os.path.join(output_dir, f'word_sources_report_{gender}.txt')
        trace_similar_word_sources(models, None, df, target_words, output_file)

# ============================================================
# ë©”ì¸ ì‹¤í–‰ë¶€ (ì•ˆì „ ê²½ë¡œ ë° í•„í„°ë§ ì ìš© ë²„ì „)
# ============================================================


def print_gender_distribution(df, title="Overall"):
    """
    ë°ì´í„°í”„ë ˆì„ ë‚´ì˜ ì„±ë³„ ë¶„í¬ë¥¼ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.
    """
    print(f"--- {title} Gender Distribution ---")
    if 'Gender' in df.columns:
        dist = df['Gender'].value_counts()
        print(dist)
    else:
        print("Error: 'Gender' column not found in DataFrame.")
    print("-" * 30) 

    
def main():
    print("\n" + "=" * 60)
    print("  K-POP ê°€ì‚¬ ì‹œëŒ€ë³„ ì˜ë¯¸ ë³€í™” ë¶„ì„ ì‹œì‘")
    print("  (ë¶„ì„ ëŒ€ìƒ: ì‚¬ë‘, ì´ë³„")
    print("=" * 60)

    # 1. ë°ì´í„° ë¡œë“œ ë° ì •ì œ
    print("\n[Step 1] ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ ë° ì •ì œ")
    df = load_and_preprocess_kpop('C:\\Users\\User\\Desktop\\DSL\\eda\\kpop_lyrics_analysis-main\\final_dataset') 

    if df is None:
        print("âŒ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return None, None, None, None

    # [ì¤‘ìš”] íŒŒì¼ëª… ì—ëŸ¬ ë°©ì§€ë¥¼ ìœ„í•´ ì„±ë³„ ê°’ ë‚´ì˜ ìŠ¬ë˜ì‹œ(/)ë¥¼ ì–¸ë”ë°”(_)ë¡œ ì¹˜í™˜
    if 'Gender' in df.columns:
        df['Gender'] = df['Gender'].replace('mixed/unknown', 'mixed_unknown')

    # 1b. ì„±ë³„ ë¶„í¬ ì¶œë ¥
    print("\n[Step 1b] Gender Distribution")
    print_gender_distribution(df, title="Overall")
    for name, start, end in ERA_DEFS:
        era_df = df[(df['Year'] >= start) & (df['Year'] <= end)]
        print_gender_distribution(era_df, title=name)

    # 2. ì‹œëŒ€ë³„/ì„±ë³„ ë¶„í• 
    print("\n[Step 2] ë°ì´í„° ë¶„í•  (ì‹œëŒ€ ë° ì„±ë³„)")
    eras = slice_by_era(df)
    gender_groups = slice_by_era_and_gender(df, gender_col='Gender')

    # 3. ëª¨ë¸ í•™ìŠµ (Word2Vec)
    print("\n[Step 3] Word2Vec ëª¨ë¸ í•™ìŠµ ì§„í–‰ ì¤‘...")
    models = train_all_models(eras)
    gender_models = train_all_models(gender_groups)

    # 4. ë¶„ì„ ëŒ€ìƒ ë‹¨ì–´ í™•ì • (23ê°œ)
    target_words = [
       'ì‚¬ë‘','ì´ë³„'
    ]

    # 4a. ì‹œëŒ€ë³„ ì˜ë¯¸ ë³€í™” ë¶„ì„ ì¶œë ¥
    print("\n[Step 4] ì‹œëŒ€ë³„ ì˜ë¯¸ ë³€í™” ë¶„ì„ ê²°ê³¼")
    for word in target_words:
        analyze_semantic_shift(models, word)

    # 4b. ì„±ë³„ ê¸°ë°˜ ì˜ë¯¸ ë³€í™” ë¶„ì„ ì¶œë ¥ (ë¦¬í¬íŠ¸ì—ëŠ” ì €ì¥ë˜ë‚˜ í™”ë©´ ì¶œë ¥ì€ ì„ íƒì‚¬í•­)
    print("\n[Step 4b] ì„±ë³„ ê¸°ë°˜ ì˜ë¯¸ ë³€í™” ë¶„ì„ ì§„í–‰ ì¤‘...")

    # 5. ì‹œê°í™” ë° ë¦¬í¬íŠ¸ íŒŒì¼ ì €ì¥
    print("\n[Step 5] ì‹œê°í™” ë° ë¦¬í¬íŠ¸ íŒŒì¼ ìƒì„± ì¤‘...")
    os.makedirs('output', exist_ok=True)
    
    # ì‹œëŒ€ë³„ t-SNE ì‹œê°í™”
    for word in target_words:
        try:
            plot_semantic_field(word, models, save_path=f'output/tsne_{word}.png')
        except Exception as e:
            print(f"  âš ï¸ '{word}' ì‹œëŒ€ë³„ ì‹œê°í™” ì‹¤íŒ¨: {e}")

    # ì„±ë³„ ê¸°ë°˜ t-SNE ì‹œê°í™” (ìˆ˜ì •ëœ í•¨ìˆ˜ í˜¸ì¶œ: mixed/unknown ì œì™¸ ë¡œì§ í¬í•¨)
    generate_gender_tsne_plots(gender_models, target_words, output_dir='output')

    # ë¦¬í¬íŠ¸ ìƒì„±
    generate_report(models, target_words, 'output/analysis_report.txt')
    append_gender_report('output/analysis_report.txt', gender_models, target_words)
    
    # ì„±ë³„ ë¦¬í¬íŠ¸ ê°œë³„ ìƒì„± (ìˆ˜ì •ëœ í•¨ìˆ˜ í˜¸ì¶œ: mixed/unknown ì œì™¸ ë¡œì§ í¬í•¨)
    generate_gender_reports(gender_models, target_words, output_dir='output')

    # 6. ìœ ì‚¬ ë‹¨ì–´ ì¶œì²˜ ì¶”ì  (ì›ë¬¸ ê°€ì‚¬ ë§¤ì¹­)
    print("\n[Step 6] ì›ë¬¸ ê°€ì‚¬ ê¸°ë°˜ ì¶œì²˜ ì¶”ì  ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
    trace_similar_word_sources(models, eras, df, target_words, 'output/word_sources_report.txt')
    generate_gender_word_sources(gender_models, df, target_words, output_dir='output')

    print("\n" + "=" * 60)
    print("  ğŸ‰ ë¶„ì„ ì™„ë£Œ! ëª¨ë“  ê²°ê³¼ëŠ” 'output' í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("=" * 60)

    return models, eras, gender_models, df

# ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
if __name__ == "__main__":
    models, eras, gender_models, df = main()

# 1. í•œê¸€ í°íŠ¸ ì„¤ì • (Windows/Mac ê³µìš©)
def set_korean_font():
    if os.name == 'nt': # ìœˆë„ìš°
        font_path = "C:/Windows/Fonts/malgun.ttf"
        font_name = font_manager.FontProperties(fname=font_path).get_name()
        rc('font', family=font_name)
        return font_path
    else: # ë§¥
        rc('font', family='AppleGothic')
        return "/System/Library/Fonts/Supplemental/AppleGothic.ttf"

# 2. ë¦¬í¬íŠ¸ì—ì„œ ë‹¨ì–´ ë­‰ì¹˜ ì¶”ì¶œ í•¨ìˆ˜
def get_all_words_from_report(filename, keyword):
    if not os.path.exists(filename):
        # íŒŒì¼ì´ ì—†ì„ ê²½ìš° (ì˜ˆ: mixed_unknown ì œì™¸ ë“±)ë¥¼ ëŒ€ë¹„í•œ ì˜ˆì™¸ ì²˜ë¦¬
        return ""
    
    with open(filename, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # í‚¤ì›Œë“œë³„ ì„¹ì…˜ ë¶„ë¦¬ (ì„¹ì…˜ êµ¬ë¶„ì 'â–  ' ê¸°ì¤€)
    sections = re.split(r'â–  ', content)
    combined_words = []
    
    for section in sections:
        if f"'{keyword}'" in section:
            # í•´ë‹¹ í‚¤ì›Œë“œ ì„¹ì…˜ ë‚´ì˜ ìœ ì‚¬ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸(ê° Eraë³„)ë¥¼ ëª¨ë‘ ì¶”ì¶œ
            # ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ [Era...] ì´í›„ ë“±ì¥í•˜ëŠ” ë‹¨ì–´ë“¤ì„ ê¸ì–´ì˜µë‹ˆë‹¤.
            matches = re.findall(r'\[.*?\]\n\s+(.*?)\n', section)
            for m in matches:
                # ì‰¼í‘œë¥¼ ì œê±°í•˜ê³  ê³µë°±ìœ¼ë¡œ ë¶„ë¦¬í•˜ì—¬ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                combined_words.extend(m.replace(',', '').split())
                
    return " ".join(combined_words)

# 3. ì›Œë“œí´ë¼ìš°ë“œ ì‹œê°í™” ì‹¤í–‰
font_p = set_korean_font()
output_dir = 'output' # ë¦¬í¬íŠ¸ê°€ ì €ì¥ëœ ê²½ë¡œ

# ìš°ë¦¬ê°€ ìµœì¢… í™•ì •í•œ 23ê°œ íƒ€ê²Ÿ ë‹¨ì–´ ì¤‘ ì£¼ìš” ë‹¨ì–´ ì„ íƒ (í˜¹ì€ ì „ì²´ ìˆœíšŒ)
# ì‹œê°í™”í•˜ê³  ì‹¶ì€ ë‹¨ì–´ë§Œ ì¶”ë ¤ë„ ë˜ê³ , target_wordsë¥¼ ê·¸ëŒ€ë¡œ ì¨ë„ ë©ë‹ˆë‹¤.
selected_keywords = [
      'ì‚¬ë‘','ì´ë³„'
    ]

for kw in selected_keywords:
    # ì„±ë³„ ë¦¬í¬íŠ¸ ê²½ë¡œ ì„¤ì • (íŒŒì¼ëª… ì•ˆì „í™” ë¡œì§ ë°˜ì˜)
    male_report = os.path.join(output_dir, 'analysis_report_male.txt')
    female_report = os.path.join(output_dir, 'analysis_report_female.txt')
    
    male_text = get_all_words_from_report(male_report, kw)
    female_text = get_all_words_from_report(female_report, kw)
    
    # ë°ì´í„°ê°€ ë¶€ì¡±í•œ ê²½ìš° ìŠ¤í‚µ
    if not male_text.strip() or not female_text.strip():
        print(f"âš ï¸ '{kw}'ì— ëŒ€í•œ ì„±ë³„ ë¹„êµ ë°ì´í„°ê°€ ë¶€ì¡±í•˜ì—¬ ê±´ë„ˆëœë‹ˆë‹¤.")
        continue

    # ì›Œë“œí´ë¼ìš°ë“œ ê°ì²´ ìƒì„±
    wc_params = {
        'font_path': font_p,
        'background_color': 'white',
        'width': 600,
        'height': 600,
        'max_words': 100,
        'prefer_horizontal': 0.9
    }
    
    wc_male = WordCloud(**wc_params, colormap='Blues').generate(male_text)
    wc_female = WordCloud(**wc_params, colormap='Reds').generate(female_text)

    # ì‹œê°í™” ë ˆì´ì•„ì›ƒ
    fig, axes = plt.subplots(1, 2, figsize=(16, 8))
    
    axes[0].imshow(wc_male, interpolation='bilinear')
    axes[0].set_title(f"ë‚¨ì„± ì•„í‹°ìŠ¤íŠ¸(Male): '{kw}'", fontsize=20, color='blue', pad=20)
    axes[0].axis('off')
    
    axes[1].imshow(wc_female, interpolation='bilinear')
    axes[1].set_title(f"ì—¬ì„± ì•„í‹°ìŠ¤íŠ¸(Female): '{kw}'", fontsize=20, color='red', pad=20)
    axes[1].axis('off')

    plt.suptitle(f"K-POP 30ë…„ ì„±ë³„ ê°€ì‚¬ ì˜ë¯¸ì¥ ëŒ€ì¡°: '{kw}'", fontsize=26, fontweight='bold', y=1.05)
    
    # ê²°ê³¼ ì €ì¥
    save_name = f"{output_dir}/wordcloud_{kw}_gender_comparison.png"
    plt.tight_layout()
    plt.savefig(save_name, dpi=300, bbox_inches='tight')
    plt.show()
    
    print(f"âœ… ì™„ë£Œ: {save_name} ì €ì¥ë¨.")



# 1. í•œê¸€ í°íŠ¸ ì„¤ì • (ê¸°ì¡´ê³¼ ë™ì¼)
def set_korean_font():
    if os.name == 'nt':
        font_path = "C:/Windows/Fonts/malgun.ttf"
        font_name = font_manager.FontProperties(fname=font_path).get_name()
        rc('font', family=font_name)
        return font_path
    else:
        rc('font', family='AppleGothic')
        return "/System/Library/Fonts/Supplemental/AppleGothic.ttf"

# 2. ë¦¬í¬íŠ¸ì—ì„œ [ì‹œëŒ€ë³„] ë‹¨ì–´ ë­‰ì¹˜ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
def get_era_words_from_report(filename, keyword):
    if not os.path.exists(filename):
        return {}
    
    with open(filename, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # í•´ë‹¹ í‚¤ì›Œë“œ ì„¹ì…˜ë§Œ ì¶”ì¶œ
    sections = re.split(r'â–  ', content)
    target_section = ""
    for section in sections:
        if f"'{keyword}'" in section:
            target_section = section
            break
            
    if not target_section:
        return {}

    # ì‹œëŒ€ë³„ë¡œ ë‹¨ì–´ ë§¤ì¹­ (Era1, Era2, Era3 ì¶”ì¶œ)
    era_data = {}
    # [Era...] ì´í›„ ë‹¤ìŒ [Era...] í˜¹ì€ ì„¹ì…˜ ë ì „ê¹Œì§€ì˜ ë‹¨ì–´ë“¤ì„ ê¸ì–´ì˜´
    matches = re.findall(r'\[(Era\d.*?)\]\n\s+(.*?)\n', target_section)
    
    for era_name, words in matches:
        era_data[era_name] = words.replace(',', '')
        
    return era_data

# 3. ì‹œê°í™” ì‹¤í–‰
font_p = set_korean_font()
output_dir = 'output'
selected_keywords = ['ì‚¬ë‘', 'ì´ë³„']
eras = ['Era1', 'Era2', 'Era3'] # ì‹œëŒ€ ë¦¬ìŠ¤íŠ¸

for kw in selected_keywords:
    male_report = os.path.join(output_dir, 'analysis_report_male.txt')
    female_report = os.path.join(output_dir, 'analysis_report_female.txt')
    
    male_era_data = get_era_words_from_report(male_report, kw)
    female_era_data = get_era_words_from_report(female_report, kw)

    # 3í–‰(ì‹œëŒ€) 2ì—´(ì„±ë³„) ë ˆì´ì•„ì›ƒ ìƒì„±
    fig, axes = plt.subplots(3, 2, figsize=(15, 18))
    
    wc_params = {
        'font_path': font_p, 'background_color': 'white',
        'width': 400, 'height': 400, 'max_words': 50
    }

    for idx, era in enumerate(eras):
        # í•´ë‹¹ ì—ë¼ ëª…ì¹­ì´ í¬í•¨ëœ í‚¤ ì°¾ê¸° (ì˜ˆ: 'Era1 (1996-2005)')
        m_key = [k for k in male_era_data.keys() if era in k]
        f_key = [k for k in female_era_data.keys() if era in k]
        
        # ë‚¨ì„± ì›Œë“œí´ë¼ìš°ë“œ
        if m_key and male_era_data[m_key[0]].strip():
            wc_m = WordCloud(**wc_params, colormap='Blues').generate(male_era_data[m_key[0]])
            axes[idx, 0].imshow(wc_m, interpolation='bilinear')
            axes[idx, 0].set_title(f"MALE | {m_key[0]}", fontsize=15, color='blue')
        axes[idx, 0].axis('off')

        # ì—¬ì„± ì›Œë“œí´ë¼ìš°ë“œ
        if f_key and female_era_data[f_key[0]].strip():
            wc_f = WordCloud(**wc_params, colormap='Reds').generate(female_era_data[f_key[0]])
            axes[idx, 1].imshow(wc_f, interpolation='bilinear')
            axes[idx, 1].set_title(f"FEMALE | {f_key[0]}", fontsize=15, color='red')
        axes[idx, 1].axis('off')

    plt.suptitle(f"K-POP 30ë…„ '{kw}' ì˜ë¯¸ ë³€í™” (ì‹œëŒ€ë³„/ì„±ë³„ ëŒ€ì¡°)", fontsize=22, fontweight='bold', y=1.02)
    
    save_name = f"{output_dir}/wordcloud_{kw}_timeline_comparison.png"
    plt.tight_layout()
    plt.savefig(save_name, dpi=300, bbox_inches='tight')
    plt.show()

passive_seeds = [
    # ê¸°ë‹¤ë¦¼ / ìˆ˜ìš©
    "ê¸°ë‹¤ë¦¬ë‹¤", "ë²„í‹°ë‹¤", "ê²¬ë””ë‹¤", "ì°¸ë‹¤", "ë§¡ê¸°ë‹¤",

    # ìƒì‹¤ / í›„í‡´
    "ë†“ì¹˜ë‹¤", "ìƒë‹¤", "ë– ë‚˜ë³´ë‚´ë‹¤", "ë©€ì–´ì§€ë‹¤", "ì‚¬ë¼ì§€ë‹¤",

    # íšŒí”¼ / ë¨¸ë¬´ë¦„
    "ìˆ¨ê¸°ë‹¤", "í”¼í•˜ë‹¤", "ë¨¸ë­‡ê±°ë¦¬ë‹¤", "ë§ì„¤ì´ë‹¤", "ë‚¨ê¸°ë‹¤"
]

active_seeds = [
    # ì ‘ê·¼ / ì‹œë„
    "ë‹¤ê°€ê°€ë‹¤", "ì¡ë‹¤", "ë§Œë‚˜ë‹¤", "ë§í•˜ë‹¤", "ê³ ë°±í•˜ë‹¤",

    # ê²°ì • / ì„ íƒ
    "ê²°ì •í•˜ë‹¤", "ì„ íƒí•˜ë‹¤", "ì •í•˜ë‹¤", "ëŠë‹¤", "ë°”ê¾¸ë‹¤",

    # ì£¼ë„ / í‘œí˜„
    "ì§€í‚¤ë‹¤", "ì™¸ì¹˜ë‹¤", "ìš”êµ¬í•˜ë‹¤", "ì´ëŒë‹¤", "ì‹œì‘í•˜ë‹¤"
]
def compare_gender_attitude(gender_models, target_word='ì‚¬ë‘'):
    eras = ['Era1 (1996-2005)', 'Era2 (2006-2015)', 'Era3 (2016-2025)']
    results = []

    for era in eras:
        for gender in ['male', 'female']:
            model = gender_models.get(f"{era} | {gender}")
            if not model or target_word not in model.wv:
                continue
            
            # í•´ë‹¹ ì„±ë³„/ì‹œëŒ€ì˜ 'ì‚¬ë‘' ìœ ì‚¬ì–´ 10ê°œ
            similar_words = [w for w, s in model.wv.most_similar(target_word, topn=10)]
            
            # ìˆ˜ë™ì„±/ëŠ¥ë™ì„± ì ìˆ˜ ê³„ì‚°
            p_score = np.mean([model.wv.similarity(word, p) for word in similar_words for p in passive_seeds if p in model.wv])
            a_score = np.mean([model.wv.similarity(word, a) for word in similar_words for a in active_seeds if a in model.wv])
            
            results.append({
                'ì‹œëŒ€': era,
                'ì„±ë³„': gender,
                'ìˆ˜ë™ì„±(Passive)': p_score,
                'ëŠ¥ë™ì„±(Active)': a_score,
                'ì£¼ì²´ì„± ì§€ìˆ˜(A-P)': a_score - p_score
            })

    return pd.DataFrame(results)

# ì‹¤í–‰
gender_diff_df = compare_gender_attitude(gender_models)
display(gender_diff_df)

def compare_gender_attitude(gender_models, target_word='ì´ë³„'):
    eras = ['Era1 (1996-2005)', 'Era2 (2006-2015)', 'Era3 (2016-2025)']
    results = []

    for era in eras:
        for gender in ['male', 'female']:
            model = gender_models.get(f"{era} | {gender}")
            if not model or target_word not in model.wv:
                continue
            
            # í•´ë‹¹ ì„±ë³„/ì‹œëŒ€ì˜ 'ì‚¬ë‘' ìœ ì‚¬ì–´ 10ê°œ
            similar_words = [w for w, s in model.wv.most_similar(target_word, topn=10)]
            
            # ìˆ˜ë™ì„±/ëŠ¥ë™ì„± ì ìˆ˜ ê³„ì‚°
            p_score = np.mean([model.wv.similarity(word, p) for word in similar_words for p in passive_seeds if p in model.wv])
            a_score = np.mean([model.wv.similarity(word, a) for word in similar_words for a in active_seeds if a in model.wv])
            
            results.append({
                'ì‹œëŒ€': era,
                'ì„±ë³„': gender,
                'ìˆ˜ë™ì„±(Passive)': p_score,
                'ëŠ¥ë™ì„±(Active)': a_score,
                'ì£¼ì²´ì„± ì§€ìˆ˜(A-P)': a_score - p_score
            })

    return pd.DataFrame(results)

# ì‹¤í–‰
gender_diff_df = compare_gender_attitude(gender_models)
display(gender_diff_df)


# ë°ì´í„° ì„¤ì • (ì—¬ì„± ì „ìš©)
eras = ['Era1', 'Era2', 'Era3']
love_female = [-0.0288, 0.0052, 0.0533]    # 'ì‚¬ë‘' ì£¼ì²´ì„± ì§€ìˆ˜
breakup_female = [-0.0642, -0.0240, 0.0092] # 'ì´ë³„' ì£¼ì²´ì„± ì§€ìˆ˜

# 2ê°œì˜ ê·¸ë˜í”„ë¥¼ ì„¸ë¡œë¡œ ë°°ì¹˜ (1í–‰ 2ì—´ë¡œ í•˜ê³  ì‹¶ë‹¤ë©´ subplots(1, 2)ë¡œ ìˆ˜ì •)
fig, axes = plt.subplots(2, 1, figsize=(10, 12))

# --- ì²« ë²ˆì§¸ ê·¸ë˜í”„: 'ì‚¬ë‘' ì£¼ì²´ì„± ë³€í™” ---
axes[0].plot(eras, love_female, marker='o', color='darkorange', linewidth=3, markersize=10)
axes[0].axhline(0, color='black', linestyle='--', linewidth=1.5) # ê¸°ì¤€ì„ 
axes[0].set_title("Female Agency Index: 'Love'", fontsize=16, pad=15)
axes[0].set_ylabel("Agency Index (A-P)", fontsize=12)
axes[0].set_ylim(-0.08, 0.08)
axes[0].grid(True, linestyle=':', alpha=0.6)
# ìˆ˜ì¹˜ í‘œì‹œ
for i, txt in enumerate(love_female):
    axes[0].annotate(f'{txt:.4f}', (eras[i], love_female[i]), textcoords="offset points", xytext=(0,10), ha='center', fontweight='bold')

# --- ë‘ ë²ˆì§¸ ê·¸ë˜í”„: 'ì´ë³„' ì£¼ì²´ì„± ë³€í™” ---
axes[1].plot(eras, breakup_female, marker='s', color='crimson', linewidth=3, markersize=10)
axes[1].axhline(0, color='black', linestyle='--', linewidth=1.5) # ê¸°ì¤€ì„ 
axes[1].set_title("Female Agency Index: 'Breakup'", fontsize=16, pad=15)
axes[1].set_ylabel("Agency Index (A-P)", fontsize=12)
axes[1].set_ylim(-0.08, 0.08)
axes[1].grid(True, linestyle=':', alpha=0.6)
# ìˆ˜ì¹˜ í‘œì‹œ
for i, txt in enumerate(breakup_female):
    axes[1].annotate(f'{txt:.4f}', (eras[i], breakup_female[i]), textcoords="offset points", xytext=(0,10), ha='center', fontweight='bold')

plt.tight_layout()
plt.savefig('female_agency_separated.png', dpi=300)
plt.show()

# 1. ë°ì´í„° ì„¤ì •
eras = ['Era1', 'Era2', 'Era3']

# [ë©”ì¸] ì—¬ì„± ê°€ì‚¬ ë°ì´í„° (ì´ì „ ë¶„ì„ ê²°ê³¼)
love_female = [-0.0288, 0.0052, 0.0533]
breakup_female = [-0.0642, -0.0240, 0.0092]

# [ë°°ê²½] ì „ì²´ ì‹œëŒ€ë³„ ì£¼ì²´ì„± ì§€ìˆ˜ (ë°©ê¸ˆ ì£¼ì‹  ë°ì´í„°)
overall_agency = [-0.028816, 0.005243, 0.053326]

# 2. ê·¸ë˜í”„ ìƒì„± (2í–‰ 1ì—´)
fig, axes = plt.subplots(2, 1, figsize=(10, 12))

# --- ì²« ë²ˆì§¸ ê·¸ë˜í”„: 'ì‚¬ë‘' (ì—¬ì„± vs ì „ì²´) ---
# ì „ì²´ íë¦„ (í¬ë¯¸í•œ ì„ )
axes[0].plot(eras, overall_agency, color='gray', linestyle='--', alpha=0.3, label='Overall Trend')
# ì—¬ì„± íë¦„ (ì§„í•œ ì„ )
axes[0].plot(eras, love_female, marker='o', color='darkorange', linewidth=3, markersize=10, label='Female: Love')

axes[0].axhline(0, color='black', linestyle='-', linewidth=1)
axes[0].set_title("Female 'Love' Agency vs. Overall Trend", fontsize=16, pad=15)
axes[0].set_ylabel("Agency Index (A-P)", fontsize=12)
axes[0].legend()
axes[0].grid(True, linestyle=':', alpha=0.6)

# --- ë‘ ë²ˆì§¸ ê·¸ë˜í”„: 'ì´ë³„' (ì—¬ì„± vs ì „ì²´) ---
# ì „ì²´ íë¦„ (í¬ë¯¸í•œ ì„ )
axes[1].plot(eras, overall_agency, color='gray', linestyle='--', alpha=0.3, label='Overall Trend')
# ì—¬ì„± íë¦„ (ì§„í•œ ì„ )
axes[1].plot(eras, breakup_female, marker='s', color='crimson', linewidth=3, markersize=10, label='Female: Breakup')

axes[1].axhline(0, color='black', linestyle='-', linewidth=1)
axes[1].set_title("Female 'Breakup' Agency vs. Overall Trend", fontsize=16, pad=15)
axes[1].set_ylabel("Agency Index (A-P)", fontsize=12)
axes[1].legend()
axes[1].grid(True, linestyle=':', alpha=0.6)

plt.tight_layout()
plt.savefig('female_vs_overall_comparison.png', dpi=300)
plt.show()
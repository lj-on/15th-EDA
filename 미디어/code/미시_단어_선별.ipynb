{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c533854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "  K-POP 가사 시대별 의미 변화 분석\n",
      "  (Melon Chart 1996-2025, 전처리 데이터)\n",
      "============================================================\n",
      "\n",
      "[Step 1] 전처리된 데이터 로드\n",
      "✓ 총 31,471개 로드\n",
      "  - 연도 범위: 1996 ~ 2025\n",
      "  (중복 제거는 시대별로 수행)\n",
      "\n",
      "[Step 2] 시대별 데이터 분할\n",
      "\n",
      "✓ 시대별 데이터 분할 (시대 내 중복 제거):\n",
      "  - Era1 (1996-2005): 7,771 → 2,224곡 (중복 5,547개 제거)\n",
      "  - Era2 (2006-2015): 11,792 → 4,799곡 (중복 6,993개 제거)\n",
      "  - Era3 (2016-2025): 11,908 → 2,307곡 (중복 9,601개 제거)\n",
      "\n",
      "[Step 3] 시대별 Word2Vec 모델 학습\n",
      "\n",
      "[Era1 (1996-2005)] 모델 학습 시작...\n",
      "  - 곡 수 필터(≥7곡): 6,635 → 1,537 단어\n",
      "  ✓ 학습 완료! (어휘 수: 1,537개)\n",
      "\n",
      "[Era2 (2006-2015)] 모델 학습 시작...\n",
      "  - 곡 수 필터(≥7곡): 11,260 → 2,707 단어\n",
      "  ✓ 학습 완료! (어휘 수: 2,707개)\n",
      "\n",
      "[Era3 (2016-2025)] 모델 학습 시작...\n",
      "  - 곡 수 필터(≥7곡): 9,900 → 2,097 단어\n",
      "  ✓ 학습 완료! (어휘 수: 2,097개)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "시대별 K-POP 가사 의미 변화 분석 (Semantic Shift Analysis)\n",
    "=============================================================\n",
    "전처리된 멜론 차트 데이터(1996-2025) 활용\n",
    "\"\"\"\n",
    "import os\n",
    "import glob\n",
    "import ast\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Step 1. 전처리된 데이터 로드\n",
    "# ============================================================\n",
    "\n",
    "def load_processed_data(lyrics_dir='processed_lyrics'):\n",
    "    \"\"\"전처리된 Excel 파일들을 병합\"\"\"\n",
    "    all_files = glob.glob(os.path.join(lyrics_dir, 'Processed_Melon_Chart_*.xlsx'))\n",
    "\n",
    "    df_list = []\n",
    "    for file in all_files:\n",
    "        df = pd.read_excel(file)\n",
    "        df_list.append(df)\n",
    "\n",
    "    combined_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    print(f\"✓ 총 {len(combined_df):,}개 로드\")\n",
    "    print(f\"  - 연도 범위: {combined_df['Year'].min()} ~ {combined_df['Year'].max()}\")\n",
    "    print(f\"  (중복 제거는 시대별로 수행)\")\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "\n",
    "def parse_tokens(token_str):\n",
    "    \"\"\"문자열 형태의 토큰 리스트를 파싱\"\"\"\n",
    "    if pd.isna(token_str):\n",
    "        return []\n",
    "    try:\n",
    "        # 문자열을 리스트로 변환\n",
    "        tokens = ast.literal_eval(token_str)\n",
    "        # 한글 토큰만 필터링 (영어/의성어 제거)\n",
    "        korean_tokens = [t for t in tokens if is_valid_korean(t)]\n",
    "        return korean_tokens\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "\n",
    "def is_valid_korean(word):\n",
    "    \"\"\"유효한 한글 단어인지 확인\"\"\"\n",
    "    # 한글이 포함되어 있는지 확인\n",
    "    has_korean = any('가' <= c <= '힣' for c in word)\n",
    "    if not has_korean:\n",
    "        return False\n",
    "    # 의성어/반복어 필터링\n",
    "    if is_onomatopoeia(word):\n",
    "        return False\n",
    "    # 불용어 필터링\n",
    "    if word in STOPWORDS:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def is_onomatopoeia(word):\n",
    "    \"\"\"의성어/반복 어구 감지\"\"\"\n",
    "    if len(word) < 2:\n",
    "        return False\n",
    "    # 동일 글자 반복 (바바바, 라라라)\n",
    "    if len(set(word)) == 1:\n",
    "        return True\n",
    "    # 2글자 반복 (짠짠, 둠칫둠칫)\n",
    "    if len(word) >= 4 and len(word) % 2 == 0:\n",
    "        half = len(word) // 2\n",
    "        if word[:half] == word[half:]:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# 불용어 리스트\n",
    "STOPWORDS = {\n",
    "    '하다', '되다', '있다', '없다', '같다', '보다', '주다', '오다', '가다',\n",
    "    '이다', '아니다', '않다', '못하다', '싶다', '알다', '모르다',\n",
    "    '그렇다', '어떻다', '이렇다', '저렇다', '이러다', '저러다' , '이번', '저번',\n",
    "    '나', '너', '너희', '우리', '저', '그', '이것', '그것', '저것',\n",
    "    '때', '것', '수', '듯', '더', '또', '다시', '너무', '정말', '진짜',\n",
    "    '아이고', '훨씬', '전혀', '몹시', '매우', '존나', '미처', '완전', '완전히', '일단',\n",
    "    '일단', '막상' , '그새', '아예', '어쨌든', '이내', '부디', '제발', '결코'\n",
    "}\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Step 2. 시대별 데이터 분할 및 모델 학습\n",
    "# ============================================================\n",
    "\n",
    "def slice_by_era(df):\n",
    "    \"\"\"데이터를 3개 시대로 분할 (시대별 중복 제거)\"\"\"\n",
    "    era_ranges = [\n",
    "        ('Era1 (1996-2005)', 1996, 2005),\n",
    "        ('Era2 (2006-2015)', 2006, 2015),\n",
    "        ('Era3 (2016-2025)', 2016, 2025),\n",
    "    ]\n",
    "\n",
    "    eras = {}\n",
    "    print(\"\\n✓ 시대별 데이터 분할 (시대 내 중복 제거):\")\n",
    "\n",
    "    for name, year_start, year_end in era_ranges:\n",
    "        era_df = df[(df['Year'] >= year_start) & (df['Year'] <= year_end)]\n",
    "        original = len(era_df)\n",
    "\n",
    "        # 시대 내에서 중복 곡 제거\n",
    "        if 'Song_ID' in era_df.columns:\n",
    "            era_df = era_df.drop_duplicates(subset=['Song_ID'], keep='first')\n",
    "\n",
    "        deduped = len(era_df)\n",
    "        removed = original - deduped\n",
    "        eras[name] = era_df\n",
    "        print(f\"  - {name}: {original:,} → {deduped:,}곡 (중복 {removed:,}개 제거)\")\n",
    "\n",
    "    return eras\n",
    "\n",
    "\n",
    "def train_era_model(era_df, era_name, vector_size=100, window=4,\n",
    "                    min_count=5, min_doc_count=7):\n",
    "    \"\"\"시대별 Word2Vec 모델 학습\n",
    "    - min_count: 전체에서 최소 등장 횟수\n",
    "    - min_doc_count: 최소 등장 곡 수\n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "\n",
    "    print(f\"\\n[{era_name}] 모델 학습 시작...\")\n",
    "\n",
    "    # 전처리된 토큰 파싱\n",
    "    tokenized_data = []\n",
    "    for tokens_str in era_df['Processed_Tokens']:\n",
    "        tokens = parse_tokens(tokens_str)\n",
    "        if len(tokens) >= 3:\n",
    "            tokenized_data.append(tokens)\n",
    "\n",
    "    if len(tokenized_data) < 10:\n",
    "        print(f\"  ⚠ 데이터 부족 (토큰화된 곡: {len(tokenized_data)})\")\n",
    "        return None\n",
    "\n",
    "    # 단어별 등장 곡 수(document frequency) 계산\n",
    "    doc_freq = Counter()\n",
    "    for tokens in tokenized_data:\n",
    "        unique_tokens = set(tokens)  # 곡 내 중복 제거\n",
    "        doc_freq.update(unique_tokens)\n",
    "\n",
    "    # min_doc_count 미만인 단어 제거\n",
    "    rare_words = {w for w, cnt in doc_freq.items() if cnt < min_doc_count}\n",
    "    filtered_data = [\n",
    "        [w for w in tokens if w not in rare_words]\n",
    "        for tokens in tokenized_data\n",
    "    ]\n",
    "    filtered_data = [t for t in filtered_data if len(t) >= 3]\n",
    "\n",
    "    before_vocab = len(set(w for tokens in tokenized_data for w in tokens))\n",
    "    after_vocab = len(set(w for tokens in filtered_data for w in tokens))\n",
    "    print(f\"  - 곡 수 필터(≥{min_doc_count}곡): {before_vocab:,} → {after_vocab:,} 단어\")\n",
    "\n",
    "    # Word2Vec 학습\n",
    "    model = Word2Vec(\n",
    "        sentences=filtered_data,\n",
    "        vector_size=vector_size,\n",
    "        window=window,\n",
    "        min_count=min_count,\n",
    "        workers=4,\n",
    "        sg=1,  # Skip-gram\n",
    "        epochs=10\n",
    "    )\n",
    "\n",
    "    print(f\"  ✓ 학습 완료! (어휘 수: {len(model.wv):,}개)\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_all_models(eras):\n",
    "    \"\"\"모든 시대별 모델 학습\"\"\"\n",
    "    models = {}\n",
    "    for name, data in eras.items():\n",
    "        model = train_era_model(data, name)\n",
    "        if model:\n",
    "            models[name] = model\n",
    "    return models\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Step 3. 의미 변화 분석\n",
    "# ============================================================\n",
    "\n",
    "def analyze_semantic_shift(models, target_word, topn=15):\n",
    "    \"\"\"특정 단어의 시대별 의미 변화 분석\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"  '{target_word}'의 시대별 의미 변화 분석\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for name, model in models.items():\n",
    "        try:\n",
    "            similar_words = model.wv.most_similar(target_word, topn=topn)\n",
    "            words_only = [w[0] for w in similar_words]\n",
    "\n",
    "            results[name] = {\n",
    "                'similar_words': similar_words,\n",
    "                'words_list': words_only\n",
    "            }\n",
    "\n",
    "            print(f\"\\n[{name}]\")\n",
    "            print(f\"  유사 단어: {', '.join(words_only[:10])}\")\n",
    "\n",
    "        except KeyError:\n",
    "            print(f\"\\n[{name}] '{target_word}'가 이 시대 데이터에 없습니다.\")\n",
    "            results[name] = None\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def find_word_sources(df, target_word, era_range=None, max_results=10):\n",
    "    \"\"\"특정 단어가 포함된 원문 가사 출처 확인\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"  '{target_word}' 단어 출처 분석\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    if era_range:\n",
    "        filtered_df = df[(df['Year'] >= era_range[0]) & (df['Year'] <= era_range[1])]\n",
    "        print(f\"  분석 범위: {era_range[0]} ~ {era_range[1]}년\")\n",
    "    else:\n",
    "        filtered_df = df\n",
    "\n",
    "    matches = []\n",
    "    for idx, row in filtered_df.iterrows():\n",
    "        tokens_str = str(row.get('Processed_Tokens', ''))\n",
    "        if target_word in tokens_str:\n",
    "            context = extract_context(str(row.get('Lyrics', '')), target_word)\n",
    "            matches.append({\n",
    "                'Year': row['Year'],\n",
    "                'Title': row['Title'],\n",
    "                'Artist': row['Artist'],\n",
    "                'Context': context\n",
    "            })\n",
    "\n",
    "    # 연도별 빈도\n",
    "    year_counts = {}\n",
    "    for m in matches:\n",
    "        year = m['Year']\n",
    "        year_counts[year] = year_counts.get(year, 0) + 1\n",
    "\n",
    "    print(f\"\\n  총 {len(matches)}곡에서 '{target_word}' 발견\")\n",
    "\n",
    "    if year_counts:\n",
    "        print(f\"\\n  [연도별 빈도]\")\n",
    "        for year in sorted(year_counts.keys()):\n",
    "            bar = '█' * min(year_counts[year], 30)\n",
    "            print(f\"  {year}: {bar} ({year_counts[year]}곡)\")\n",
    "\n",
    "    print(f\"\\n  [대표 곡 목록]\")\n",
    "    for i, m in enumerate(matches[:max_results]):\n",
    "        print(f\"  {i+1}. [{m['Year']}] {m['Artist']} - {m['Title']}\")\n",
    "        if m['Context']:\n",
    "            print(f\"      \\\"{m['Context'][:60]}...\\\"\")\n",
    "\n",
    "    return matches\n",
    "\n",
    "\n",
    "def extract_context(lyrics, word, window=30):\n",
    "    \"\"\"단어 주변 문맥 추출\"\"\"\n",
    "    if not lyrics:\n",
    "        return \"\"\n",
    "    idx = lyrics.find(word)\n",
    "    if idx == -1:\n",
    "        return \"\"\n",
    "    start = max(0, idx - window)\n",
    "    end = min(len(lyrics), idx + len(word) + window)\n",
    "    return lyrics[start:end].replace('\\n', ' ')\n",
    "\n",
    "\n",
    "def deep_dive_word(models, df, target_word, era_name=None):\n",
    "    \"\"\"특정 단어 심층 분석\"\"\"\n",
    "    print(f\"\\n{'#'*60}\")\n",
    "    print(f\"  '{target_word}' 심층 분석\")\n",
    "    print(f\"{'#'*60}\")\n",
    "\n",
    "    # 유사 단어 분석\n",
    "    if era_name and era_name in models:\n",
    "        model = models[era_name]\n",
    "        try:\n",
    "            similar = model.wv.most_similar(target_word, topn=15)\n",
    "            print(f\"\\n  [{era_name}] '{target_word}'와 유사한 단어:\")\n",
    "            for w, score in similar:\n",
    "                print(f\"    - {w}: {score:.3f}\")\n",
    "        except KeyError:\n",
    "            print(f\"  '{target_word}'가 해당 시대에 없습니다.\")\n",
    "    else:\n",
    "        for name, model in models.items():\n",
    "            try:\n",
    "                similar = model.wv.most_similar(target_word, topn=5)\n",
    "                words_str = ', '.join([f\"{w}({s:.2f})\" for w, s in similar])\n",
    "                print(f\"\\n  [{name}] {words_str}\")\n",
    "            except KeyError:\n",
    "                print(f\"\\n  [{name}] 없음\")\n",
    "\n",
    "    # 출처 확인\n",
    "    if era_name:\n",
    "        import re\n",
    "        years = re.findall(r'\\d{4}', era_name)\n",
    "        if len(years) == 2:\n",
    "            find_word_sources(df, target_word, era_range=(int(years[0]), int(years[1])), max_results=5)\n",
    "    else:\n",
    "        find_word_sources(df, target_word, max_results=10)\n",
    "\n",
    "# ============================================================\n",
    "# 메인 실행\n",
    "# ============================================================\n",
    "\n",
    "def main():\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"  K-POP 가사 시대별 의미 변화 분석\")\n",
    "    print(\"  (Melon Chart 1996-2025, 전처리 데이터)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # 1. 전처리된 데이터 로드\n",
    "    print(\"\\n[Step 1] 전처리된 데이터 로드\")\n",
    "    df = load_processed_data('processed_lyrics')\n",
    "\n",
    "    # 2. 시대별 분할\n",
    "    print(\"\\n[Step 2] 시대별 데이터 분할\")\n",
    "    eras = slice_by_era(df)\n",
    "\n",
    "    # 3. 모델 학습\n",
    "    print(\"\\n[Step 3] 시대별 Word2Vec 모델 학습\")\n",
    "    models = train_all_models(eras)\n",
    "\n",
    "    return models, eras, df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    models, eras, df = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfea57e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "진행 중: Era1 (1996-2005) vs Era2 (2006-2015)...\n",
      "진행 중: Era2 (2006-2015) vs Era3 (2016-2025)...\n",
      "\n",
      "============================================================\n",
      "  ★ K-POP 시대별 순차적 의미 변화 TOP 30 ★\n",
      "  (비교 구간: Era1 (1996-2005)->Era2 (2006-2015) 및 Era2 (2006-2015)->Era3 (2016-2025))\n",
      "============================================================\n",
      " 1. 어떡         (합계 변화량: 1.3001)\n",
      "    [Era1 (1996-2005)->Era2 (2006-2015): 0.660] -> [Era2 (2006-2015)->Era3 (2016-2025): 0.640]\n",
      "-------------------------------------------------------\n",
      " 2. 널          (합계 변화량: 1.2890)\n",
      "    [Era1 (1996-2005)->Era2 (2006-2015): 0.712] -> [Era2 (2006-2015)->Era3 (2016-2025): 0.577]\n",
      "-------------------------------------------------------\n",
      " 3. 어설프다       (합계 변화량: 1.2369)\n",
      "    [Era1 (1996-2005)->Era2 (2006-2015): 0.582] -> [Era2 (2006-2015)->Era3 (2016-2025): 0.655]\n",
      "-------------------------------------------------------\n",
      " 4. 세우다        (합계 변화량: 1.2361)\n",
      "    [Era1 (1996-2005)->Era2 (2006-2015): 0.639] -> [Era2 (2006-2015)->Era3 (2016-2025): 0.597]\n",
      "-------------------------------------------------------\n",
      " 5. 거부         (합계 변화량: 1.2196)\n",
      "    [Era1 (1996-2005)->Era2 (2006-2015): 0.496] -> [Era2 (2006-2015)->Era3 (2016-2025): 0.723]\n",
      "-------------------------------------------------------\n",
      " 6. 분명히        (합계 변화량: 1.2174)\n",
      "    [Era1 (1996-2005)->Era2 (2006-2015): 0.573] -> [Era2 (2006-2015)->Era3 (2016-2025): 0.645]\n",
      "-------------------------------------------------------\n",
      " 7. 충분히        (합계 변화량: 1.2089)\n",
      "    [Era1 (1996-2005)->Era2 (2006-2015): 0.628] -> [Era2 (2006-2015)->Era3 (2016-2025): 0.581]\n",
      "-------------------------------------------------------\n",
      " 8. 피하다        (합계 변화량: 1.1994)\n",
      "    [Era1 (1996-2005)->Era2 (2006-2015): 0.664] -> [Era2 (2006-2015)->Era3 (2016-2025): 0.535]\n",
      "-------------------------------------------------------\n",
      " 9. 자연         (합계 변화량: 1.1909)\n",
      "    [Era1 (1996-2005)->Era2 (2006-2015): 0.597] -> [Era2 (2006-2015)->Era3 (2016-2025): 0.594]\n",
      "-------------------------------------------------------\n",
      "10. 아니         (합계 변화량: 1.1899)\n",
      "    [Era1 (1996-2005)->Era2 (2006-2015): 0.618] -> [Era2 (2006-2015)->Era3 (2016-2025): 0.572]\n",
      "-------------------------------------------------------\n",
      "11. 철없다        (합계 변화량: 1.1891)\n",
      "    [Era1 (1996-2005)->Era2 (2006-2015): 0.593] -> [Era2 (2006-2015)->Era3 (2016-2025): 0.596]\n",
      "-------------------------------------------------------\n",
      "12. 고프다        (합계 변화량: 1.1793)\n",
      "    [Era1 (1996-2005)->Era2 (2006-2015): 0.546] -> [Era2 (2006-2015)->Era3 (2016-2025): 0.633]\n",
      "-------------------------------------------------------\n",
      "13. 가르치다       (합계 변화량: 1.1752)\n",
      "    [Era1 (1996-2005)->Era2 (2006-2015): 0.570] -> [Era2 (2006-2015)->Era3 (2016-2025): 0.605]\n",
      "-------------------------------------------------------\n",
      "14. 복잡         (합계 변화량: 1.1720)\n",
      "    [Era1 (1996-2005)->Era2 (2006-2015): 0.618] -> [Era2 (2006-2015)->Era3 (2016-2025): 0.554]\n",
      "-------------------------------------------------------\n",
      "15. 키스         (합계 변화량: 1.1712)\n",
      "    [Era1 (1996-2005)->Era2 (2006-2015): 0.471] -> [Era2 (2006-2015)->Era3 (2016-2025): 0.701]\n",
      "-------------------------------------------------------\n",
      "16. 네          (합계 변화량: 1.1591)\n",
      "    [Era1 (1996-2005)->Era2 (2006-2015): 0.623] -> [Era2 (2006-2015)->Era3 (2016-2025): 0.536]\n",
      "-------------------------------------------------------\n",
      "17. 지금껏        (합계 변화량: 1.1577)\n",
      "    [Era1 (1996-2005)->Era2 (2006-2015): 0.611] -> [Era2 (2006-2015)->Era3 (2016-2025): 0.546]\n",
      "-------------------------------------------------------\n",
      "18. 갚다         (합계 변화량: 1.1566)\n",
      "    [Era1 (1996-2005)->Era2 (2006-2015): 0.514] -> [Era2 (2006-2015)->Era3 (2016-2025): 0.642]\n",
      "-------------------------------------------------------\n",
      "19. 만지다        (합계 변화량: 1.1557)\n",
      "    [Era1 (1996-2005)->Era2 (2006-2015): 0.571] -> [Era2 (2006-2015)->Era3 (2016-2025): 0.585]\n",
      "-------------------------------------------------------\n",
      "20. 떨다         (합계 변화량: 1.1552)\n",
      "    [Era1 (1996-2005)->Era2 (2006-2015): 0.581] -> [Era2 (2006-2015)->Era3 (2016-2025): 0.575]\n",
      "-------------------------------------------------------\n",
      "21. 문제         (합계 변화량: 1.1550)\n",
      "    [Era1 (1996-2005)->Era2 (2006-2015): 0.469] -> [Era2 (2006-2015)->Era3 (2016-2025): 0.686]\n",
      "-------------------------------------------------------\n",
      "22. 밀다         (합계 변화량: 1.1546)\n",
      "    [Era1 (1996-2005)->Era2 (2006-2015): 0.583] -> [Era2 (2006-2015)->Era3 (2016-2025): 0.572]\n",
      "-------------------------------------------------------\n",
      "23. 한번         (합계 변화량: 1.1540)\n",
      "    [Era1 (1996-2005)->Era2 (2006-2015): 0.580] -> [Era2 (2006-2015)->Era3 (2016-2025): 0.574]\n",
      "-------------------------------------------------------\n",
      "24. 멋대로        (합계 변화량: 1.1518)\n",
      "    [Era1 (1996-2005)->Era2 (2006-2015): 0.519] -> [Era2 (2006-2015)->Era3 (2016-2025): 0.633]\n",
      "-------------------------------------------------------\n",
      "25. 넘기다        (합계 변화량: 1.1456)\n",
      "    [Era1 (1996-2005)->Era2 (2006-2015): 0.476] -> [Era2 (2006-2015)->Era3 (2016-2025): 0.670]\n",
      "-------------------------------------------------------\n",
      "26. 붙이다        (합계 변화량: 1.1413)\n",
      "    [Era1 (1996-2005)->Era2 (2006-2015): 0.543] -> [Era2 (2006-2015)->Era3 (2016-2025): 0.599]\n",
      "-------------------------------------------------------\n",
      "27. 끌다         (합계 변화량: 1.1377)\n",
      "    [Era1 (1996-2005)->Era2 (2006-2015): 0.601] -> [Era2 (2006-2015)->Era3 (2016-2025): 0.537]\n",
      "-------------------------------------------------------\n",
      "28. 수없이        (합계 변화량: 1.1368)\n",
      "    [Era1 (1996-2005)->Era2 (2006-2015): 0.525] -> [Era2 (2006-2015)->Era3 (2016-2025): 0.612]\n",
      "-------------------------------------------------------\n",
      "29. 머릿속        (합계 변화량: 1.1367)\n",
      "    [Era1 (1996-2005)->Era2 (2006-2015): 0.586] -> [Era2 (2006-2015)->Era3 (2016-2025): 0.550]\n",
      "-------------------------------------------------------\n",
      "30. 거두다        (합계 변화량: 1.1298)\n",
      "    [Era1 (1996-2005)->Era2 (2006-2015): 0.541] -> [Era2 (2006-2015)->Era3 (2016-2025): 0.589]\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.linalg import orthogonal_procrustes\n",
    "\n",
    "def align_and_compare(model_ref, model_target):\n",
    "    \"\"\"\n",
    "    [엔진] 두 모델을 정렬하고 단어별 코사인 유사도(의미 변화)를 계산\n",
    "    \"\"\"\n",
    "    common_vocab = list(set(model_ref.wv.index_to_key) & set(model_target.wv.index_to_key))\n",
    "    if not common_vocab:\n",
    "        return pd.DataFrame(columns=['word', 'shift_score'])\n",
    "\n",
    "    mat_ref = np.vstack([model_ref.wv[word] for word in common_vocab])\n",
    "    mat_target = np.vstack([model_target.wv[word] for word in common_vocab])\n",
    "\n",
    "    # Procrustes 정렬: 두 벡터 공간의 좌표축을 맞춤\n",
    "    R, _ = orthogonal_procrustes(mat_target, mat_ref)\n",
    "    aligned_target_mat = mat_target @ R\n",
    "\n",
    "    shifts = []\n",
    "    for i, word in enumerate(common_vocab):\n",
    "        vec_ref = mat_ref[i]\n",
    "        vec_aligned = aligned_target_mat[i]\n",
    "        \n",
    "        # 코사인 유사도 계산\n",
    "        sim = np.dot(vec_ref, vec_aligned) / (np.linalg.norm(vec_ref) * np.linalg.norm(vec_aligned))\n",
    "        # 1 - 유사도 = 변화량 (높을수록 의미가 많이 변함)\n",
    "        shifts.append({'word': word, 'shift_score': 1 - sim})\n",
    "\n",
    "    return pd.DataFrame(shifts)\n",
    "\n",
    "def get_multi_era_top_shifts(models, top_n=30):\n",
    "    \"\"\"\n",
    "    [분석기] Era 1->2, Era 2->3의 변화를 순차적으로 비교하여 누적 변화량 계산\n",
    "    \"\"\"\n",
    "    era_names = list(models.keys())\n",
    "    if len(era_names) < 3:\n",
    "        print(\"⚠ 분석을 위해 최소 3개의 시대별 모델이 필요합니다.\")\n",
    "        return []\n",
    "\n",
    "    # 1. 시기별 변화량 계산 (1-2 구간, 2-3 구간)\n",
    "    print(f\"진행 중: {era_names[0]} vs {era_names[1]}...\")\n",
    "    shift_1_2 = align_and_compare(models[era_names[0]], models[era_names[1]])\n",
    "    \n",
    "    print(f\"진행 중: {era_names[1]} vs {era_names[2]}...\")\n",
    "    shift_2_3 = align_and_compare(models[era_names[1]], models[era_names[2]])\n",
    "\n",
    "    # 2. 데이터 병합 및 누적 점수 계산\n",
    "    total_shift = pd.merge(shift_1_2, shift_2_3, on='word', suffixes=('_12', '_23'))\n",
    "    total_shift['total_shift_score'] = total_shift['shift_score_12'] + total_shift['shift_score_23']\n",
    "    \n",
    "    # 3. 정렬 및 결과 출력\n",
    "    total_shift = total_shift.sort_values('total_shift_score', ascending=False)\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(f\"  ★ K-POP 시대별 순차적 의미 변화 TOP {top_n} ★\")\n",
    "    print(f\"  (비교 구간: {era_names[0]}->{era_names[1]} 및 {era_names[1]}->{era_names[2]})\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for i, row in enumerate(total_shift.head(top_n).itertuples(), 1):\n",
    "        print(f\"{i:2d}. {row.word:<10} (합계 변화량: {row.total_shift_score:.4f})\")\n",
    "        print(f\"    [{era_names[0]}->{era_names[1]}: {row.shift_score_12:.3f}] -> [{era_names[1]}->{era_names[2]}: {row.shift_score_23:.3f}]\")\n",
    "        print(\"-\" * 55)\n",
    "    \n",
    "    return total_shift\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 실행 (models 객체가 준비된 상태에서)\n",
    "# ---------------------------------------------------------\n",
    "result_df = get_multi_era_top_shifts(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7364197a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Era1 (1996-2005) vs Era2 (2006-2015)] 분석 중...\n",
      "[Era2 (2006-2015) vs Era3 (2016-2025)] 분석 중...\n",
      "\n",
      "============================================================\n",
      "✓ 분석 결과 저장 완료: output/sequential_semantic_shifts.csv\n",
      "★ Era 1->2->3 누적 변화 TOP 30 ★\n",
      "============================================================\n",
      " 1. 어떡         (합계: 1.3001)\n",
      "    [E1->E2: 0.660] [E2->E3: 0.640]\n",
      " 2. 널          (합계: 1.2890)\n",
      "    [E1->E2: 0.712] [E2->E3: 0.577]\n",
      " 3. 어설프다       (합계: 1.2369)\n",
      "    [E1->E2: 0.582] [E2->E3: 0.655]\n",
      " 4. 세우다        (합계: 1.2361)\n",
      "    [E1->E2: 0.639] [E2->E3: 0.597]\n",
      " 5. 거부         (합계: 1.2196)\n",
      "    [E1->E2: 0.496] [E2->E3: 0.723]\n",
      " 6. 분명히        (합계: 1.2174)\n",
      "    [E1->E2: 0.573] [E2->E3: 0.645]\n",
      " 7. 충분히        (합계: 1.2089)\n",
      "    [E1->E2: 0.628] [E2->E3: 0.581]\n",
      " 8. 피하다        (합계: 1.1994)\n",
      "    [E1->E2: 0.664] [E2->E3: 0.535]\n",
      " 9. 자연         (합계: 1.1909)\n",
      "    [E1->E2: 0.597] [E2->E3: 0.594]\n",
      "10. 아니         (합계: 1.1899)\n",
      "    [E1->E2: 0.618] [E2->E3: 0.572]\n",
      "11. 철없다        (합계: 1.1891)\n",
      "    [E1->E2: 0.593] [E2->E3: 0.596]\n",
      "12. 고프다        (합계: 1.1793)\n",
      "    [E1->E2: 0.546] [E2->E3: 0.633]\n",
      "13. 가르치다       (합계: 1.1752)\n",
      "    [E1->E2: 0.570] [E2->E3: 0.605]\n",
      "14. 복잡         (합계: 1.1720)\n",
      "    [E1->E2: 0.618] [E2->E3: 0.554]\n",
      "15. 키스         (합계: 1.1712)\n",
      "    [E1->E2: 0.471] [E2->E3: 0.701]\n",
      "16. 네          (합계: 1.1591)\n",
      "    [E1->E2: 0.623] [E2->E3: 0.536]\n",
      "17. 지금껏        (합계: 1.1577)\n",
      "    [E1->E2: 0.611] [E2->E3: 0.546]\n",
      "18. 갚다         (합계: 1.1566)\n",
      "    [E1->E2: 0.514] [E2->E3: 0.642]\n",
      "19. 만지다        (합계: 1.1557)\n",
      "    [E1->E2: 0.571] [E2->E3: 0.585]\n",
      "20. 떨다         (합계: 1.1552)\n",
      "    [E1->E2: 0.581] [E2->E3: 0.575]\n",
      "21. 문제         (합계: 1.1550)\n",
      "    [E1->E2: 0.469] [E2->E3: 0.686]\n",
      "22. 밀다         (합계: 1.1546)\n",
      "    [E1->E2: 0.583] [E2->E3: 0.572]\n",
      "23. 한번         (합계: 1.1540)\n",
      "    [E1->E2: 0.580] [E2->E3: 0.574]\n",
      "24. 멋대로        (합계: 1.1518)\n",
      "    [E1->E2: 0.519] [E2->E3: 0.633]\n",
      "25. 넘기다        (합계: 1.1456)\n",
      "    [E1->E2: 0.476] [E2->E3: 0.670]\n",
      "26. 붙이다        (합계: 1.1413)\n",
      "    [E1->E2: 0.543] [E2->E3: 0.599]\n",
      "27. 끌다         (합계: 1.1377)\n",
      "    [E1->E2: 0.601] [E2->E3: 0.537]\n",
      "28. 수없이        (합계: 1.1368)\n",
      "    [E1->E2: 0.525] [E2->E3: 0.612]\n",
      "29. 머릿속        (합계: 1.1367)\n",
      "    [E1->E2: 0.586] [E2->E3: 0.550]\n",
      "30. 거두다        (합계: 1.1298)\n",
      "    [E1->E2: 0.541] [E2->E3: 0.589]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from scipy.linalg import orthogonal_procrustes\n",
    "\n",
    "def align_and_compare(model_ref, model_target):\n",
    "    \"\"\"\n",
    "    [엔진] 두 모델을 정렬하고 단어별 코사인 유사도(의미 변화)를 계산\n",
    "    \"\"\"\n",
    "    common_vocab = list(set(model_ref.wv.index_to_key) & set(model_target.wv.index_to_key))\n",
    "    if not common_vocab:\n",
    "        return pd.DataFrame(columns=['word', 'shift_score'])\n",
    "\n",
    "    mat_ref = np.vstack([model_ref.wv[word] for word in common_vocab])\n",
    "    mat_target = np.vstack([model_target.wv[word] for word in common_vocab])\n",
    "\n",
    "    # Procrustes 정렬\n",
    "    R, _ = orthogonal_procrustes(mat_target, mat_ref)\n",
    "    aligned_target_mat = mat_target @ R\n",
    "\n",
    "    shifts = []\n",
    "    for i, word in enumerate(common_vocab):\n",
    "        vec_ref = mat_ref[i]\n",
    "        vec_aligned = aligned_target_mat[i]\n",
    "        sim = np.dot(vec_ref, vec_aligned) / (np.linalg.norm(vec_ref) * np.linalg.norm(vec_aligned))\n",
    "        shifts.append({'word': word, 'shift_score': 1 - sim})\n",
    "\n",
    "    return pd.DataFrame(shifts)\n",
    "\n",
    "def get_sequential_shift_and_save(models, top_n=30, output_file='output/sequential_semantic_shifts.csv'):\n",
    "    \"\"\"\n",
    "    [분석기] Era 1->2, Era 2->3 순차적 변화량 계산 및 CSV 저장\n",
    "    \"\"\"\n",
    "    era_names = list(models.keys())\n",
    "    if len(era_names) < 3:\n",
    "        print(\"⚠ 3개 이상의 시대 모델이 필요합니다.\")\n",
    "        return None\n",
    "\n",
    "    # 1. 구간별 변화량 계산 (1-2, 2-3)\n",
    "    print(f\"[{era_names[0]} vs {era_names[1]}] 분석 중...\")\n",
    "    s12 = align_and_compare(models[era_names[0]], models[era_names[1]])\n",
    "    \n",
    "    print(f\"[{era_names[1]} vs {era_names[2]}] 분석 중...\")\n",
    "    s23 = align_and_compare(models[era_names[1]], models[era_names[2]])\n",
    "\n",
    "    # 2. 데이터 병합 및 누적 점수 계산\n",
    "    total = pd.merge(s12, s23, on='word', suffixes=('_12', '_23'))\n",
    "    total['total_shift_score'] = total['shift_score_12'] + total['shift_score_23']\n",
    "    \n",
    "    # 3. 정렬 및 순위 부여\n",
    "    total = total.sort_values('total_shift_score', ascending=False).reset_index(drop=True)\n",
    "    total.index = total.index + 1 # 순위를 1부터 시작하게 설정\n",
    "\n",
    "    # 4. CSV 저장\n",
    "    os.makedirs('output', exist_ok=True)\n",
    "    total.to_csv(output_file, encoding='utf-8-sig', index_label='순위')\n",
    "    \n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(f\"✓ 분석 결과 저장 완료: {output_file}\")\n",
    "    print(f\"★ Era 1->2->3 누적 변화 TOP {top_n} ★\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for i, row in enumerate(total.head(top_n).itertuples(), 1):\n",
    "        print(f\"{i:2d}. {row.word:<10} (합계: {row.total_shift_score:.4f})\")\n",
    "        print(f\"    [E1->E2: {row.shift_score_12:.3f}] [E2->E3: {row.shift_score_23:.3f}]\")\n",
    "    \n",
    "    return total\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 실행\n",
    "# ---------------------------------------------------------\n",
    "result_df = get_sequential_shift_and_save(models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "8c020584",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[RUN] StdContact -> C:\\Users\\asap0\\OneDrive\\바탕 화면\\yonsei\\26-1 DSL\\eda\\SemiConductor_EDA_2602\\LithoBench\\0_Datasets\\lithodata\\StdContact\n",
            "[OK] common masks = 163\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[5], line 299\u001b[0m\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 299\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATASETS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    300\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, df \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    301\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[HEAD] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[1;32mIn[5], line 294\u001b[0m, in \u001b[0;36mrun_all\u001b[1;34m(datasets)\u001b[0m\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[RUN] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mroot\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    293\u001b[0m     cfg \u001b[38;5;241m=\u001b[39m make_cfg(name, root)\n\u001b[1;32m--> 294\u001b[0m     results[name] \u001b[38;5;241m=\u001b[39m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
            "Cell \u001b[1;32mIn[5], line 276\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(cfg)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(common, \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    275\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 276\u001b[0m         rows\u001b[38;5;241m.\u001b[39mappend(\u001b[43mprocess_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    278\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[ERROR] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[1;32mIn[5], line 232\u001b[0m, in \u001b[0;36mprocess_one\u001b[1;34m(name, maps, cfg)\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;66;03m# --- Stage-wise: target vs (glp, pixelILT, printed)\u001b[39;00m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;66;03m# row[\"iou_target_glp\"] = iou(bins[\"target\"], bins[\"glp\"])\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;66;03m# row[\"xor_target_glp\"] = xor_rate(bins[\"target\"], bins[\"glp\"])\u001b[39;00m\n\u001b[0;32m    231\u001b[0m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miou_target_pixelILT\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m iou(bins[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m], bins[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixelILT\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m--> 232\u001b[0m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxor_target_pixelILT\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mxor_rate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbins\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtarget\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbins\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpixelILT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    234\u001b[0m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miou_target_printed\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m iou(bins[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m], bins[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprinted\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    235\u001b[0m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxor_target_printed\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m xor_rate(bins[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m], bins[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprinted\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
            "Cell \u001b[1;32mIn[5], line 175\u001b[0m, in \u001b[0;36mxor_rate\u001b[1;34m(a, b)\u001b[0m\n\u001b[0;32m    173\u001b[0m a \u001b[38;5;241m=\u001b[39m (a \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    174\u001b[0m b \u001b[38;5;241m=\u001b[39m (b \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 175\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogical_xor\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m    176\u001b[0m uni \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlogical_or(a, b)\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(x \u001b[38;5;241m/\u001b[39m (uni \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-8\u001b[39m))\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    root: str\n",
        "    folders: Tuple[str, ...] = (\"litho\", \"pixelILT\", \"printed\", \"resist\", \"target\")\n",
        "\n",
        "    # binary vs gray stages\n",
        "    binary_stages: Tuple[str, ...] = (\"target\", \"pixelILT\", \"printed\")\n",
        "    gray_stages: Tuple[str, ...] = (\"litho\", \"resist\")\n",
        "\n",
        "    thresholds: Tuple[float, ...] = (0.30, 0.40, 0.50)\n",
        "\n",
        "    # Morph cleanup kernel size (set 0 to disable)\n",
        "    morph_k: int = 3\n",
        "\n",
        "    out_csv: str = \"\"\n",
        "\n",
        "\n",
        "DATASETS: Dict[str, str] = {\n",
        "    \"StdContact\": \"C:\\\\Users\\\\asap0\\\\OneDrive\\\\바탕 화면\\\\yonsei\\\\26-1 DSL\\\\eda\\\\SemiConductor_EDA_2602\\\\LithoBench\\\\0_Datasets\\\\lithodata\\\\StdContact\",\n",
        "    \"StdContactTest\": \"C:\\\\Users\\\\asap0\\\\OneDrive\\\\바탕 화면\\\\yonsei\\\\26-1 DSL\\\\eda\\\\SemiConductor_EDA_2602\\\\LithoBench\\\\0_Datasets\\\\lithodata\\\\StdContactTest\",\n",
        "    \"ViaSet\": \"C:\\\\Users\\\\asap0\\\\OneDrive\\\\바탕 화면\\\\yonsei\\\\26-1 DSL\\\\eda\\\\SemiConductor_EDA_2602\\\\LithoBench\\\\0_Datasets\\\\lithodata\\\\ViaSet\",\n",
        "}\n",
        "\n",
        "\n",
        "def make_cfg(name: str, root: str) -> Config:\n",
        "    return Config(\n",
        "        root=root,\n",
        "        out_csv=f\"eda_6stage_features_errors_{name}.csv\",\n",
        "    )\n",
        "\n",
        "\n",
        "#File indexing (filename-based matching)\n",
        "def list_files(folder: Path) -> Dict[str, Path]:\n",
        "    return {p.name: p for p in folder.iterdir() if p.is_file()}\n",
        "\n",
        "\n",
        "def build_index(cfg: Config) -> Tuple[Dict[str, Dict[str, Path]], List[str]]:\n",
        "    root = Path(cfg.root)\n",
        "    if not root.exists():\n",
        "        raise FileNotFoundError(f\"Root not found: {root.resolve()}\")\n",
        "\n",
        "    maps: Dict[str, Dict[str, Path]] = {}\n",
        "    for f in cfg.folders:\n",
        "        fp = root / f\n",
        "        if not fp.exists():\n",
        "            raise FileNotFoundError(f\"Missing folder: {fp.resolve()}\")\n",
        "        maps[f] = list_files(fp)\n",
        "\n",
        "    common = set(maps[cfg.folders[0]].keys())\n",
        "    for f in cfg.folders[1:]:\n",
        "        common &= set(maps[f].keys())\n",
        "    common = sorted(common)\n",
        "\n",
        "    if len(common) == 0:\n",
        "        raise RuntimeError(\"No common filenames across all 6 folders.\")\n",
        "\n",
        "    # quick report\n",
        "    for f in cfg.folders:\n",
        "        miss = sorted(set(common) - set(maps[f].keys()))\n",
        "        extra = sorted(set(maps[f].keys()) - set(common))\n",
        "        if miss:\n",
        "            print(f\"[WARN] {f}: missing {len(miss)} (first 5): {miss[:5]}\")\n",
        "        if extra:\n",
        "            print(f\"[INFO] {f}: extra {len(extra)} (first 5): {extra[:5]}\")\n",
        "\n",
        "    print(f\"[OK] common masks = {len(common)}\")\n",
        "    return maps, common\n",
        "\n",
        "\n",
        "#Robust loader (png/jpg + npy)\n",
        "def load_as_float01(path: Path) -> np.ndarray:\n",
        "    suf = path.suffix.lower()\n",
        "    if suf == \".npy\":\n",
        "        arr = np.load(path)\n",
        "        arr = np.asarray(arr)\n",
        "        if arr.ndim == 3:\n",
        "            arr = arr.mean(axis=-1)\n",
        "        arr = arr.astype(np.float32)\n",
        "        mn, mx = float(arr.min()), float(arr.max())\n",
        "        if mx - mn < 1e-8:\n",
        "            return np.zeros_like(arr, dtype=np.float32)\n",
        "        return (arr - mn) / (mx - mn)\n",
        "\n",
        "    # png/jpg\n",
        "    if not path.exists() or path.stat().st_size == 0:\n",
        "        raise ValueError(f\"Missing or empty file: {path}\")\n",
        "\n",
        "    img = cv2.imread(str(path), cv2.IMREAD_GRAYSCALE)\n",
        "    if img is None:\n",
        "        # fallback: PIL\n",
        "        from PIL import Image\n",
        "        with Image.open(path) as im:\n",
        "            img = np.array(im.convert(\"L\"))\n",
        "\n",
        "    return img.astype(np.float32) / 255.0\n",
        "\n",
        "\n",
        "# Part 3) Binarization + cleanup\n",
        "def otsu_binarize(img01: np.ndarray, auto_invert: bool = True) -> np.ndarray:\n",
        "    img8 = np.clip(img01 * 255.0, 0, 255).astype(np.uint8)\n",
        "    blur = cv2.GaussianBlur(img8, (5, 5), 0)\n",
        "    _, b = cv2.threshold(blur, 0, 1, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "    if auto_invert and b.mean() > 0.5:\n",
        "        b = 1 - b\n",
        "    return b.astype(np.uint8)\n",
        "\n",
        "\n",
        "def threshold_binarize(img01: np.ndarray, thr: float, auto_invert: bool = True) -> np.ndarray:\n",
        "    b = (img01 > thr).astype(np.uint8)\n",
        "    if auto_invert and b.mean() > 0.5:\n",
        "        b = 1 - b\n",
        "    return b\n",
        "\n",
        "\n",
        "def cleanup_binary(bin01: np.ndarray, k: int) -> np.ndarray:\n",
        "    if k is None or k <= 0:\n",
        "        return bin01.astype(np.uint8)\n",
        "    kernel = np.ones((k, k), np.uint8)\n",
        "    x = cv2.morphologyEx(bin01.astype(np.uint8), cv2.MORPH_OPEN, kernel)\n",
        "    x = cv2.morphologyEx(x, cv2.MORPH_CLOSE, kernel)\n",
        "    return x.astype(np.uint8)\n",
        "\n",
        "\n",
        "#Target feature extraction (geometry / complexity)\n",
        "def perimeter_px(bin01: np.ndarray) -> int:\n",
        "    edge = cv2.Canny((bin01 * 255).astype(np.uint8), 50, 150)\n",
        "    return int((edge > 0).sum())\n",
        "\n",
        "\n",
        "def count_components(bin01: np.ndarray) -> int:\n",
        "    num, _ = cv2.connectedComponents((bin01 > 0).astype(np.uint8), connectivity=4)\n",
        "    return int(num - 1)\n",
        "\n",
        "\n",
        "def extract_target_features(target_bin: np.ndarray) -> Dict[str, float]:\n",
        "    area_frac = float(target_bin.mean())\n",
        "    perim = float(perimeter_px(target_bin))\n",
        "    comps = float(count_components(target_bin))\n",
        "    area = float(target_bin.sum()) + 1e-6\n",
        "    compact = float((perim * perim) / area)\n",
        "    edge = cv2.Canny((target_bin * 255).astype(np.uint8), 50, 150)\n",
        "    edge_density = float((edge > 0).mean())\n",
        "\n",
        "    return {\n",
        "        \"target_area_frac\": area_frac,\n",
        "        \"target_perimeter\": perim,\n",
        "        \"target_components\": comps,\n",
        "        \"target_compactness\": compact,\n",
        "        \"target_edge_density\": edge_density,\n",
        "    }\n",
        "\n",
        "\n",
        "#Error metrics (IoU / XOR / component delta)\n",
        "def iou(a: np.ndarray, b: np.ndarray) -> float:\n",
        "    a = (a > 0)\n",
        "    b = (b > 0)\n",
        "    inter = np.logical_and(a, b).sum()\n",
        "    uni = np.logical_or(a, b).sum()\n",
        "    return float(inter / (uni + 1e-8))\n",
        "\n",
        "\n",
        "def xor_rate(a: np.ndarray, b: np.ndarray) -> float:\n",
        "    a = (a > 0)\n",
        "    b = (b > 0)\n",
        "    x = np.logical_xor(a, b).sum()\n",
        "    uni = np.logical_or(a, b).sum()\n",
        "    return float(x / (uni + 1e-8))\n",
        "\n",
        "\n",
        "def comp_delta(a: np.ndarray, b: np.ndarray) -> int:\n",
        "    return int(count_components(b) - count_components(a))\n",
        "\n",
        "#Optional alignment sanity check (centroid shift)\n",
        "def centroid_xy(bin01: np.ndarray) -> Tuple[float, float]:\n",
        "    ys, xs = np.where(bin01 > 0)\n",
        "    if len(xs) == 0:\n",
        "        return (np.nan, np.nan)\n",
        "    return float(xs.mean()), float(ys.mean())\n",
        "\n",
        "\n",
        "def centroid_shift(a: np.ndarray, b: np.ndarray) -> float:\n",
        "    ax, ay = centroid_xy(a)\n",
        "    bx, by = centroid_xy(b)\n",
        "    if np.isnan(ax) or np.isnan(bx):\n",
        "        return float(\"nan\")\n",
        "    return float(np.sqrt((ax - bx) ** 2 + (ay - by) ** 2))\n",
        "\n",
        "\n",
        "#Process one mask (load -> preprocess -> features/errors)\n",
        "def process_one(name: str, maps: Dict[str, Dict[str, Path]], cfg: Config) -> Dict[str, float]:\n",
        "    # Load float01\n",
        "    imgs = {st: load_as_float01(maps[st][name]) for st in cfg.folders}\n",
        "\n",
        "    # Binarize binary-stages (Otsu)\n",
        "    bins = {}\n",
        "    for st in cfg.binary_stages:\n",
        "        b = otsu_binarize(imgs[st], auto_invert=True)\n",
        "        b = cleanup_binary(b, cfg.morph_k)\n",
        "        bins[st] = b\n",
        "\n",
        "    # Gray stages -> multi-threshold bins\n",
        "    gray_bins: Dict[str, Dict[float, np.ndarray]] = {}\n",
        "    for st in cfg.gray_stages:\n",
        "        gray_bins[st] = {}\n",
        "        for t in cfg.thresholds:\n",
        "            b = threshold_binarize(imgs[st], float(t), auto_invert=True)\n",
        "            b = cleanup_binary(b, cfg.morph_k)\n",
        "            gray_bins[st][float(t)] = b\n",
        "\n",
        "    # Row: id + target features\n",
        "    row: Dict[str, float] = {\"mask_name\": name}\n",
        "    row.update(extract_target_features(bins[\"target\"]))\n",
        "\n",
        "    # Alignment sanity: target vs printed\n",
        "    row[\"shift_target_printed\"] = centroid_shift(bins[\"target\"], bins[\"printed\"])\n",
        "\n",
        "    # --- Stage-wise: target vs (glp, pixelILT, printed)\n",
        "    # row[\"iou_target_glp\"] = iou(bins[\"target\"], bins[\"glp\"])\n",
        "    # row[\"xor_target_glp\"] = xor_rate(bins[\"target\"], bins[\"glp\"])\n",
        "\n",
        "    row[\"iou_target_pixelILT\"] = iou(bins[\"target\"], bins[\"pixelILT\"])\n",
        "    row[\"xor_target_pixelILT\"] = xor_rate(bins[\"target\"], bins[\"pixelILT\"])\n",
        "\n",
        "    row[\"iou_target_printed\"] = iou(bins[\"target\"], bins[\"printed\"])\n",
        "    row[\"xor_target_printed\"] = xor_rate(bins[\"target\"], bins[\"printed\"])\n",
        "    row[\"comp_delta_target_printed\"] = comp_delta(bins[\"target\"], bins[\"printed\"])\n",
        "\n",
        "    # --- Stage-wise: (glp or pixelILT) -> litho -> resist -> printed (threshold별)\n",
        "    # glp -> litho\n",
        "    for t in cfg.thresholds:\n",
        "        lt = gray_bins[\"litho\"][float(t)]\n",
        "        # row[f\"iou_glp_litho_t{t:.2f}\"] = iou(bins[\"glp\"], lt)\n",
        "        # row[f\"xor_glp_litho_t{t:.2f}\"] = xor_rate(bins[\"glp\"], lt)\n",
        "\n",
        "        row[f\"iou_pixelILT_litho_t{t:.2f}\"] = iou(bins[\"pixelILT\"], lt)\n",
        "        row[f\"xor_pixelILT_litho_t{t:.2f}\"] = xor_rate(bins[\"pixelILT\"], lt)\n",
        "\n",
        "    # litho -> resist\n",
        "    for t in cfg.thresholds:\n",
        "        lt = gray_bins[\"litho\"][float(t)]\n",
        "        rt = gray_bins[\"resist\"][float(t)]\n",
        "        row[f\"iou_litho_resist_t{t:.2f}\"] = iou(lt, rt)\n",
        "        row[f\"xor_litho_resist_t{t:.2f}\"] = xor_rate(lt, rt)\n",
        "\n",
        "    # resist -> printed\n",
        "    for t in cfg.thresholds:\n",
        "        rt = gray_bins[\"resist\"][float(t)]\n",
        "        row[f\"iou_resist_printed_t{t:.2f}\"] = iou(rt, bins[\"printed\"])\n",
        "        row[f\"xor_resist_printed_t{t:.2f}\"] = xor_rate(rt, bins[\"printed\"])\n",
        "        row[f\"comp_delta_resist_printed_t{t:.2f}\"] = comp_delta(rt, bins[\"printed\"])\n",
        "\n",
        "    # Threshold stability summary (resist->printed)\n",
        "    rp_ious = [row[f\"iou_resist_printed_t{t:.2f}\"] for t in cfg.thresholds]\n",
        "    row[\"iou_resist_printed_mean\"] = float(np.mean(rp_ious))\n",
        "    row[\"iou_resist_printed_std\"] = float(np.std(rp_ious))\n",
        "\n",
        "    return row\n",
        "\n",
        "\n",
        "def run(cfg: Config) -> pd.DataFrame:\n",
        "    maps, common = build_index(cfg)\n",
        "\n",
        "    rows = []\n",
        "    for i, name in enumerate(common, 1):\n",
        "        try:\n",
        "            rows.append(process_one(name, maps, cfg))\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] {name}: {e}\")\n",
        "\n",
        "        if i % 50 == 0 or i == len(common):\n",
        "            print(f\"Processed {i}/{len(common)}\")\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    df.to_csv(cfg.out_csv, index=False, encoding=\"utf-8-sig\")\n",
        "    print(f\"[DONE] Saved {cfg.out_csv} | rows={len(df)} cols={df.shape[1]}\")\n",
        "    return df\n",
        "\n",
        "\n",
        "def run_all(datasets: Dict[str, str]) -> Dict[str, pd.DataFrame]:\n",
        "    results: Dict[str, pd.DataFrame] = {}\n",
        "    for name, root in datasets.items():\n",
        "        print(f\"\\n[RUN] {name} -> {root}\")\n",
        "        cfg = make_cfg(name, root)\n",
        "        results[name] = run(cfg)\n",
        "    return results\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    results = run_all(DATASETS)\n",
        "    for name, df in results.items():\n",
        "        print(f\"\\n[HEAD] {name}\")\n",
        "        print(df.head(3).to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec159e31",
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    root: str = \"C:\\\\Users\\\\asap0\\\\OneDrive\\\\바탕 화면\\\\yonsei\\\\26-1 DSL\\\\eda\\\\SemiConductor_EDA_2602\\\\LithoBench\\\\0_Datasets\\\\lithodata\\\\\" # <-- change to your path\n",
        "    folders: Tuple[str, ...] = (\"litho\", \"pixelILT\", \"printed\", \"resist\", \"target\")\n",
        "\n",
        "    # binary vs gray stages\n",
        "    binary_stages: Tuple[str, ...] = (\"target\", \"pixelILT\", \"printed\")\n",
        "    gray_stages: Tuple[str, ...] = (\"litho\", \"resist\")\n",
        "\n",
        "    thresholds: Tuple[float, ...] = (0.30, 0.40, 0.50)\n",
        "\n",
        "    # Morph cleanup kernel size (set 0 to disable)\n",
        "    morph_k: int = 3\n",
        "\n",
        "    out_csv: str = \"eda_6stage_features_errors_MetalSet.csv\"\n",
        "\n",
        "\n",
        "CFG = Config()\n",
        "\n",
        "\n",
        "#File indexing (filename-based matching)\n",
        "def list_files(folder: Path) -> Dict[str, Path]:\n",
        "    return {p.name: p for p in folder.iterdir() if p.is_file()}\n",
        "\n",
        "\n",
        "def build_index(cfg: Config) -> Tuple[Dict[str, Dict[str, Path]], List[str]]:\n",
        "    root = Path(cfg.root)\n",
        "    if not root.exists():\n",
        "        raise FileNotFoundError(f\"Root not found: {root.resolve()}\")\n",
        "\n",
        "    maps: Dict[str, Dict[str, Path]] = {}\n",
        "    for f in cfg.folders:\n",
        "        fp = root / f\n",
        "        if not fp.exists():\n",
        "            raise FileNotFoundError(f\"Missing folder: {fp.resolve()}\")\n",
        "        maps[f] = list_files(fp)\n",
        "\n",
        "    common = set(maps[cfg.folders[0]].keys())\n",
        "    for f in cfg.folders[1:]:\n",
        "        common &= set(maps[f].keys())\n",
        "    common = sorted(common)\n",
        "\n",
        "    if len(common) == 0:\n",
        "        raise RuntimeError(\"No common filenames across all 6 folders.\")\n",
        "\n",
        "    # quick report\n",
        "    for f in cfg.folders:\n",
        "        miss = sorted(set(common) - set(maps[f].keys()))\n",
        "        extra = sorted(set(maps[f].keys()) - set(common))\n",
        "        if miss:\n",
        "            print(f\"[WARN] {f}: missing {len(miss)} (first 5): {miss[:5]}\")\n",
        "        if extra:\n",
        "            print(f\"[INFO] {f}: extra {len(extra)} (first 5): {extra[:5]}\")\n",
        "\n",
        "    print(f\"[OK] common masks = {len(common)}\")\n",
        "    return maps, common\n",
        "\n",
        "\n",
        "#Robust loader (png/jpg + npy)\n",
        "def load_as_float01(path: Path) -> np.ndarray:\n",
        "    suf = path.suffix.lower()\n",
        "    if suf == \".npy\":\n",
        "        arr = np.load(path)\n",
        "        arr = np.asarray(arr)\n",
        "        if arr.ndim == 3:\n",
        "            arr = arr.mean(axis=-1)\n",
        "        arr = arr.astype(np.float32)\n",
        "        mn, mx = float(arr.min()), float(arr.max())\n",
        "        if mx - mn < 1e-8:\n",
        "            return np.zeros_like(arr, dtype=np.float32)\n",
        "        return (arr - mn) / (mx - mn)\n",
        "\n",
        "    # png/jpg\n",
        "    if not path.exists() or path.stat().st_size == 0:\n",
        "        raise ValueError(f\"Missing or empty file: {path}\")\n",
        "\n",
        "    img = cv2.imread(str(path), cv2.IMREAD_GRAYSCALE)\n",
        "    if img is None:\n",
        "        # fallback: PIL\n",
        "        from PIL import Image\n",
        "        with Image.open(path) as im:\n",
        "            img = np.array(im.convert(\"L\"))\n",
        "\n",
        "    return img.astype(np.float32) / 255.0\n",
        "\n",
        "\n",
        "# Part 3) Binarization + cleanup\n",
        "def otsu_binarize(img01: np.ndarray, auto_invert: bool = True) -> np.ndarray:\n",
        "    img8 = np.clip(img01 * 255.0, 0, 255).astype(np.uint8)\n",
        "    blur = cv2.GaussianBlur(img8, (5, 5), 0)\n",
        "    _, b = cv2.threshold(blur, 0, 1, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "    if auto_invert and b.mean() > 0.5:\n",
        "        b = 1 - b\n",
        "    return b.astype(np.uint8)\n",
        "\n",
        "\n",
        "def threshold_binarize(img01: np.ndarray, thr: float, auto_invert: bool = True) -> np.ndarray:\n",
        "    b = (img01 > thr).astype(np.uint8)\n",
        "    if auto_invert and b.mean() > 0.5:\n",
        "        b = 1 - b\n",
        "    return b\n",
        "\n",
        "\n",
        "def cleanup_binary(bin01: np.ndarray, k: int) -> np.ndarray:\n",
        "    if k is None or k <= 0:\n",
        "        return bin01.astype(np.uint8)\n",
        "    kernel = np.ones((k, k), np.uint8)\n",
        "    x = cv2.morphologyEx(bin01.astype(np.uint8), cv2.MORPH_OPEN, kernel)\n",
        "    x = cv2.morphologyEx(x, cv2.MORPH_CLOSE, kernel)\n",
        "    return x.astype(np.uint8)\n",
        "\n",
        "\n",
        "#Target feature extraction (geometry / complexity)\n",
        "def perimeter_px(bin01: np.ndarray) -> int:\n",
        "    edge = cv2.Canny((bin01 * 255).astype(np.uint8), 50, 150)\n",
        "    return int((edge > 0).sum())\n",
        "\n",
        "\n",
        "def count_components(bin01: np.ndarray) -> int:\n",
        "    num, _ = cv2.connectedComponents((bin01 > 0).astype(np.uint8), connectivity=4)\n",
        "    return int(num - 1)\n",
        "\n",
        "\n",
        "def extract_target_features(target_bin: np.ndarray) -> Dict[str, float]:\n",
        "    area_frac = float(target_bin.mean())\n",
        "    perim = float(perimeter_px(target_bin))\n",
        "    comps = float(count_components(target_bin))\n",
        "    area = float(target_bin.sum()) + 1e-6\n",
        "    compact = float((perim * perim) / area)\n",
        "    edge = cv2.Canny((target_bin * 255).astype(np.uint8), 50, 150)\n",
        "    edge_density = float((edge > 0).mean())\n",
        "\n",
        "    return {\n",
        "        \"target_area_frac\": area_frac,\n",
        "        \"target_perimeter\": perim,\n",
        "        \"target_components\": comps,\n",
        "        \"target_compactness\": compact,\n",
        "        \"target_edge_density\": edge_density,\n",
        "    }\n",
        "\n",
        "\n",
        "#Error metrics (IoU / XOR / component delta)\n",
        "def iou(a: np.ndarray, b: np.ndarray) -> float:\n",
        "    a = (a > 0)\n",
        "    b = (b > 0)\n",
        "    inter = np.logical_and(a, b).sum()\n",
        "    uni = np.logical_or(a, b).sum()\n",
        "    return float(inter / (uni + 1e-8))\n",
        "\n",
        "\n",
        "def xor_rate(a: np.ndarray, b: np.ndarray) -> float:\n",
        "    a = (a > 0)\n",
        "    b = (b > 0)\n",
        "    x = np.logical_xor(a, b).sum()\n",
        "    uni = np.logical_or(a, b).sum()\n",
        "    return float(x / (uni + 1e-8))\n",
        "\n",
        "\n",
        "def comp_delta(a: np.ndarray, b: np.ndarray) -> int:\n",
        "    return int(count_components(b) - count_components(a))\n",
        "\n",
        "#Optional alignment sanity check (centroid shift)\n",
        "def centroid_xy(bin01: np.ndarray) -> Tuple[float, float]:\n",
        "    ys, xs = np.where(bin01 > 0)\n",
        "    if len(xs) == 0:\n",
        "        return (np.nan, np.nan)\n",
        "    return float(xs.mean()), float(ys.mean())\n",
        "\n",
        "\n",
        "def centroid_shift(a: np.ndarray, b: np.ndarray) -> float:\n",
        "    ax, ay = centroid_xy(a)\n",
        "    bx, by = centroid_xy(b)\n",
        "    if np.isnan(ax) or np.isnan(bx):\n",
        "        return float(\"nan\")\n",
        "    return float(np.sqrt((ax - bx) ** 2 + (ay - by) ** 2))\n",
        "\n",
        "\n",
        "#Process one mask (load -> preprocess -> features/errors)\n",
        "def process_one(name: str, maps: Dict[str, Dict[str, Path]], cfg: Config) -> Dict[str, float]:\n",
        "    # Load float01\n",
        "    imgs = {st: load_as_float01(maps[st][name]) for st in cfg.folders}\n",
        "\n",
        "    # Binarize binary-stages (Otsu)\n",
        "    bins = {}\n",
        "    for st in cfg.binary_stages:\n",
        "        b = otsu_binarize(imgs[st], auto_invert=True)\n",
        "        b = cleanup_binary(b, cfg.morph_k)\n",
        "        bins[st] = b\n",
        "\n",
        "    # Gray stages -> multi-threshold bins\n",
        "    gray_bins: Dict[str, Dict[float, np.ndarray]] = {}\n",
        "    for st in cfg.gray_stages:\n",
        "        gray_bins[st] = {}\n",
        "        for t in cfg.thresholds:\n",
        "            b = threshold_binarize(imgs[st], float(t), auto_invert=True)\n",
        "            b = cleanup_binary(b, cfg.morph_k)\n",
        "            gray_bins[st][float(t)] = b\n",
        "\n",
        "    # Row: id + target features\n",
        "    row: Dict[str, float] = {\"mask_name\": name}\n",
        "    row.update(extract_target_features(bins[\"target\"]))\n",
        "\n",
        "    # Alignment sanity: target vs printed\n",
        "    row[\"shift_target_printed\"] = centroid_shift(bins[\"target\"], bins[\"printed\"])\n",
        "\n",
        "    # --- Stage-wise: target vs (glp, pixelILT, printed)\n",
        "    # row[\"iou_target_glp\"] = iou(bins[\"target\"], bins[\"glp\"])\n",
        "    # row[\"xor_target_glp\"] = xor_rate(bins[\"target\"], bins[\"glp\"])\n",
        "\n",
        "    row[\"iou_target_pixelILT\"] = iou(bins[\"target\"], bins[\"pixelILT\"])\n",
        "    row[\"xor_target_pixelILT\"] = xor_rate(bins[\"target\"], bins[\"pixelILT\"])\n",
        "\n",
        "    row[\"iou_target_printed\"] = iou(bins[\"target\"], bins[\"printed\"])\n",
        "    row[\"xor_target_printed\"] = xor_rate(bins[\"target\"], bins[\"printed\"])\n",
        "    row[\"comp_delta_target_printed\"] = comp_delta(bins[\"target\"], bins[\"printed\"])\n",
        "\n",
        "    # --- Stage-wise: (glp or pixelILT) -> litho -> resist -> printed (threshold별)\n",
        "    # glp -> litho\n",
        "    for t in cfg.thresholds:\n",
        "        lt = gray_bins[\"litho\"][float(t)]\n",
        "        # row[f\"iou_glp_litho_t{t:.2f}\"] = iou(bins[\"glp\"], lt)\n",
        "        # row[f\"xor_glp_litho_t{t:.2f}\"] = xor_rate(bins[\"glp\"], lt)\n",
        "\n",
        "        row[f\"iou_pixelILT_litho_t{t:.2f}\"] = iou(bins[\"pixelILT\"], lt)\n",
        "        row[f\"xor_pixelILT_litho_t{t:.2f}\"] = xor_rate(bins[\"pixelILT\"], lt)\n",
        "\n",
        "    # litho -> resist\n",
        "    for t in cfg.thresholds:\n",
        "        lt = gray_bins[\"litho\"][float(t)]\n",
        "        rt = gray_bins[\"resist\"][float(t)]\n",
        "        row[f\"iou_litho_resist_t{t:.2f}\"] = iou(lt, rt)\n",
        "        row[f\"xor_litho_resist_t{t:.2f}\"] = xor_rate(lt, rt)\n",
        "\n",
        "    # resist -> printed\n",
        "    for t in cfg.thresholds:\n",
        "        rt = gray_bins[\"resist\"][float(t)]\n",
        "        row[f\"iou_resist_printed_t{t:.2f}\"] = iou(rt, bins[\"printed\"])\n",
        "        row[f\"xor_resist_printed_t{t:.2f}\"] = xor_rate(rt, bins[\"printed\"])\n",
        "        row[f\"comp_delta_resist_printed_t{t:.2f}\"] = comp_delta(rt, bins[\"printed\"])\n",
        "\n",
        "    # Threshold stability summary (resist->printed)\n",
        "    rp_ious = [row[f\"iou_resist_printed_t{t:.2f}\"] for t in cfg.thresholds]\n",
        "    row[\"iou_resist_printed_mean\"] = float(np.mean(rp_ious))\n",
        "    row[\"iou_resist_printed_std\"] = float(np.std(rp_ious))\n",
        "\n",
        "    return row\n",
        "\n",
        "\n",
        "def run(cfg: Config) -> pd.DataFrame:\n",
        "    maps, common = build_index(cfg)\n",
        "\n",
        "    rows = []\n",
        "    for i, name in enumerate(common, 1):\n",
        "        try:\n",
        "            rows.append(process_one(name, maps, cfg))\n",
        "        except Exception as e:\n",
        "            print(f\"[ERROR] {name}: {e}\")\n",
        "\n",
        "        if i % 50 == 0 or i == len(common):\n",
        "            print(f\"Processed {i}/{len(common)}\")\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    df.to_csv(cfg.out_csv, index=False, encoding=\"utf-8-sig\")\n",
        "    print(f\"[DONE] Saved {cfg.out_csv} | rows={len(df)} cols={df.shape[1]}\")\n",
        "    return df\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    df = run(CFG)\n",
        "    print(df.head(3).to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18148f52",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
